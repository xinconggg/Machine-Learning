{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPMOurI7fUczUxI2Vp4qVLt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinconggg/Machine-Learning/blob/main/Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "Requires Python 3.7 or above:"
      ],
      "metadata": {
        "id": "NH4mo4lt8EPc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wtbXLGfR77eY"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "assert sys.version_info >= (3, 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have to use Keras 2 instead of 3. To do that, set the `TF_USE_LEGACY_KERAS` environment variable to \"1\" and import the `tf_keras` package. This ensures that `tf.keras` points to `tf_keras`, which is Keras 2.*."
      ],
      "metadata": {
        "id": "7ocqGFqY8Mnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
        "\n",
        "import tf_keras"
      ],
      "metadata": {
        "id": "FcZW-qoC8Gj-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And TensorFlow ≥ 2.8:\n",
        "\n"
      ],
      "metadata": {
        "id": "rkUa3B0q8Rf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from packaging import version\n",
        "import tensorflow as tf\n",
        "\n",
        "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")"
      ],
      "metadata": {
        "id": "bunEbj8n8Pgl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning to Optimize Rewards\n",
        "In reinforcement learning, a software agent makes observations and takes actions within an environment, and in\n",
        "return it receives rewards from the environment. Its objective is to learn to act in a way that will maximize its\n",
        "expected rewards over time. For example:\n",
        "1. The agent can be the program controlling a robot. In this case, the environment is the real world, the agent\n",
        "observes the environment through a set of sensors such as cameras and touch sensors, and its actions consist\n",
        "of sending signals to activate motors. It may be programmed to get positive rewards whenever it approaches\n",
        "the target destination, and negative rewards whenever it wastes time or goes in the wrong direction.\n",
        "2. The agent can be the program controlling Ms. Pac-Man. In this case, the environment is a simulation of the\n",
        "Atari game, the actions are the nine possible joystick positions (upper left, down, center, and so on), the\n",
        "observations are screenshots, and the rewards are just the game points.\n",
        "3. Similarly, the agent can be the program playing a board game such as Go. It only gets a reward if it wins.\n",
        "4. The agent does not have to control a physically (or virtually) moving thing. For example, it can be a smart\n",
        "thermostat, getting positive rewards whenever it is close to the target temperature and saves energy, and\n",
        "negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human\n",
        "needs.\n",
        "5. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are\n",
        "obviously the monetary gains and losses."
      ],
      "metadata": {
        "id": "4EJJQEh8EeHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to OpenAI Gym\n",
        "Let's import Gym and make a new CartPole environment:"
      ],
      "metadata": {
        "id": "Jwc7qjtVFEpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if \"google.colab\" in sys.modules or \"kaggle_secrets\" in sys.modules:\n",
        "    %pip install -q -U gymnasium swig\n",
        "    %pip install -q -U gymnasium[classic_control,box2d,atari,accept-rom-license]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PS3uXJRFohM",
        "outputId": "66a55d12-5938-4b0e-ece1-51e598c721ba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/958.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m952.3/958.1 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: gymnasium 1.0.0 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")"
      ],
      "metadata": {
        "id": "IbU8m9U38W8D"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the environment is created, we must initialize it using the `reset()` method. This returns the first observation. Observations depend on the type of environment. For the CartPole\n",
        "environment, each observation is a 1D NumPy array containing four floats representing the cart’s horizontal\n",
        "position (0.0 = center), its velocity (positive means right), the angle of the pole (0.0 = vertical), and its angular\n",
        "velocity (positive means clockwise). The `reset()` method also returns a dictionary that may contain extra\n",
        "environment-specific information. This can be useful for debugging or for training. For example, in many Atari\n",
        "environments, it contains the number of lives left.\n",
        "\n",
        "Let's initialize the environment by calling is `reset()` method:"
      ],
      "metadata": {
        "id": "GtEflRZ8GHlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "obs, info = env.reset(seed=42)\n",
        "obs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aSZcnZdFl2l",
        "outputId": "9044976a-943f-4336-93ff-1d3226b102fc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.0273956 , -0.00611216,  0.03585979,  0.0197368 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBgK0I8jG2H1",
        "outputId": "db39d2a0-9f60-4eac-bc5d-43288122d000"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s call the `render()` method to render this environment as an image. Since we set `render_mode=\"rgb_array\"`\n",
        "when creating the environment, the image will be returned as a NumPy array:"
      ],
      "metadata": {
        "id": "83EIqs3wG784"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = env.render()\n",
        "img.shape  # height, width, channels (3 = Red, Green, Blue)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Zq-PchLG4ts",
        "outputId": "47750201-ed1f-40b0-f4ac-09b594dbacff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400, 600, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Matplotlib's `imshow()` function to render and plot the an environment:"
      ],
      "metadata": {
        "id": "QFC8w6jsHJHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_environment(env, figsize=(5, 4)):\n",
        "    plt.figure(figsize=figsize)\n",
        "    img = env.render()\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    return img\n",
        "\n",
        "plot_environment(env)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "8gLd1oPSHAkh",
        "outputId": "7ad6e1eb-40bc-4d10-c1ad-d7dd8c9a8d92"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's ask the environment what actions are possible:"
      ],
      "metadata": {
        "id": "LIKuzVLoHXRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKVXsmc4HTbW",
        "outputId": "4a003d72-edcb-42c4-f844-7cf67431342a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(2)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Discrete(2)` means that the possible actions are integers 0 and 1, which represent accelerating left or right. Other\n",
        "environments may have additional discrete actions, or other kinds of actions. Since the pole is\n",
        "leaning toward the right (`obs[2] > 0`), let’s accelerate the cart toward the right:"
      ],
      "metadata": {
        "id": "44geHmxtHgEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "action = 1  # accelerate right\n",
        "obs, reward, done, truncated, info = env.step(action)\n",
        "obs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alikSLyWHavE",
        "outputId": "9f0ef1ee-4c0d-42f0-977a-15a2a9eda48f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.02727336,  0.18847767,  0.03625453, -0.26141977], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the cart is now moving toward the right (`obs[1] > 0`). The pole is still tilted toward the right (`obs[2] > 0`), but its angular velocity is now negative (`obs[3] < 0`), so it will likely be tilted toward the left after the next step."
      ],
      "metadata": {
        "id": "7-_WZ9i6H0s0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the environment:"
      ],
      "metadata": {
        "id": "nnuFwYpoH_gd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_environment(env)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "oSGxtF6zHpqB",
        "outputId": "a75d9a19-1c8c-4ee4-911b-1b02d5ce807d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The environment also tells the agent how much reward it got during the last step:\n",
        "\n"
      ],
      "metadata": {
        "id": "IzgQdsTiIJHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reward"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMUGqYIDICBU",
        "outputId": "638890a6-898f-45a9-a97b-aa34b4c8850f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the game is over, the environment returns `done=True`. In this case, it's not over yet:\n",
        "\n"
      ],
      "metadata": {
        "id": "mzy2WivqINTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "done"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYAok7FcILEi",
        "outputId": "4c4561f8-f54d-4e3b-da55-f245809ac8de"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some environment wrappers may want to interrupt the environment early. For example, when a time limit is reached or when an object goes out of bounds. In this case, `truncated` will be set to **True**. In this case, it's not truncated yet:\n",
        "\n"
      ],
      "metadata": {
        "id": "oGCOU9w9ITWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "truncated"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iHOj6pCIQB5",
        "outputId": "c4b01747-92e3-4a73-c1da-bbbf3e74aabe"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A Simple Hard-Coded Policy\n",
        "Let's hard code a simple strategy: if the pole is tilting to the left, then push the cart to the left, and vice versa. We will run this policy to see the average rewards it gets over 500\n",
        "episodes:"
      ],
      "metadata": {
        "id": "qd5fNS7zIcmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_policy(obs):\n",
        "    angle = obs[2]\n",
        "    return 0 if angle < 0 else 1\n",
        "\n",
        "totals = []\n",
        "for episode in range(500):\n",
        "    episode_rewards = 0\n",
        "    obs, info = env.reset(seed=episode)\n",
        "    for step in range(200):\n",
        "        action = basic_policy(obs)\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "        episode_rewards += reward\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "    totals.append(episode_rewards)"
      ],
      "metadata": {
        "id": "jPMcuEubIYI3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at the results:"
      ],
      "metadata": {
        "id": "fhZaulV8Isl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.mean(totals), np.std(totals), min(totals), max(totals)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DImxw_NhIp6j",
        "outputId": "fe06a572-e704-4689-a4fd-7cb8384a82cd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(41.698, 8.389445512070509, 24.0, 63.0)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, this strategy is a bit too basic: the best it did was to keep the poll up for only 63 steps. This environment is considered solved when the agent keeps the poll up for 200 steps. Even with 500 tries, this policy never managed to keep the pole upright for more than 63 consecutive steps. Not\n",
        "great. Let’s see if a neural network can come up with a better\n",
        "policy."
      ],
      "metadata": {
        "id": "iWHSMJnPIzD5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize one episode:"
      ],
      "metadata": {
        "id": "aoMyEextU0Ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.animation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def update_scene(num, frames, patch):\n",
        "    patch.set_data(frames[num])\n",
        "    return patch,\n",
        "\n",
        "def plot_animation(frames, repeat=False, interval=40):\n",
        "    fig = plt.figure()\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "    anim = matplotlib.animation.FuncAnimation(\n",
        "        fig, update_scene, fargs=(frames, patch),\n",
        "        frames=len(frames), repeat=repeat, interval=interval)\n",
        "    plt.close()\n",
        "    return anim\n",
        "\n",
        "def show_one_episode(policy, n_max_steps=200, seed=42):\n",
        "    frames = []\n",
        "    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "    np.random.seed(seed)\n",
        "    obs, info = env.reset(seed=seed)\n",
        "    for step in range(n_max_steps):\n",
        "        frames.append(env.render())\n",
        "        action = policy(obs)\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "        if done or truncated:\n",
        "            break\n",
        "    env.close()\n",
        "    return plot_animation(frames)"
      ],
      "metadata": {
        "id": "uPfmAxe5U3TM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "show_one_episode(basic_policy)"
      ],
      "metadata": {
        "id": "C8EQoj5dU70k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebf0abec-2eee-4e66-862a-9f65ece7e37a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.animation.FuncAnimation at 0x7ba19809c3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network Policies\n",
        "Let’s create a neural network policy. This neural network will take an observation as input, and it will output the\n",
        "action to be executed, just like the policy we hardcoded earlier. More precisely, it will estimate a probability for\n",
        "each action, and then we will select an action randomly, according to the estimated probabilities. In the case of the CartPole environment, there are just two possible actions (left or right), so we only need one\n",
        "output neuron. It will output the probability p of action 0 (left), and of course the probability of action 1 (right) will\n",
        "be 1 – p. For example, if it outputs 0.7, then we will pick action 0 with 70% probability, or action 1 with 30%\n",
        "probability.\n",
        "\n",
        "The following code builds a basic neural network policy using Keras:"
      ],
      "metadata": {
        "id": "Feq5-7vxJAc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(5, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "])"
      ],
      "metadata": {
        "id": "iqjg1HgMIt4T"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use a `Sequential` model to define the policy network. The number of inputs is the size of the observation\n",
        "space—which in the case of CartPole is 4—and we have just five hidden units because it’s a fairly simple task.\n",
        "Finally, we want to output a single probability—the probability of going left—so we have a single output neuron\n",
        "using the sigmoid activation function. If there were more than two possible actions, there would be one output\n",
        "neuron per action, and we would use the softmax activation function instead."
      ],
      "metadata": {
        "id": "SSYUPcUpKUT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's write a small policy function that will use the neural net to get the probability of moving left, then let's use it to run one episode:\n",
        "\n"
      ],
      "metadata": {
        "id": "fcyg4ntxVERw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pg_policy(obs):\n",
        "    left_proba = model.predict(obs[np.newaxis], verbose=0)[0][0]\n",
        "    return int(np.random.rand() > left_proba)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "show_one_episode(pg_policy)"
      ],
      "metadata": {
        "id": "SWeqQkubVGv8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a355fe20-4833-4299-f275-4da32dfb4f17"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.animation.FuncAnimation at 0x7ba197d68a60>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy Gradients (PG)\n",
        "PG algorithms optimize the parameters of a policy by following the gradients toward higher\n",
        "rewards. Below is a common variant:\n",
        "1. First, let the neural network policy play the game several times, and at each step, compute the gradients that\n",
        "would make the chosen action even more likely—but don’t apply these gradients yet.\n",
        "2. Once you have run several episodes, compute each action’s advantage, using the method described in the\n",
        "previous section.\n",
        "3. If an action’s advantage is positive, it means that the action was probably good, and you want to apply the\n",
        "gradients computed earlier to make the action even more likely to be chosen in the future. However, if the\n",
        "action’s advantage is negative, it means the action was probably bad, and you want to apply the opposite\n",
        "gradients to make this action slightly less likely in the future. The solution is to multiply each gradient vector\n",
        "by the corresponding action’s advantage.\n",
        "4. Finally, compute the mean of all the resulting gradient vectors, and use it to perform a gradient descent step."
      ],
      "metadata": {
        "id": "vyKMMvzuKgVM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s use Keras to implement this algorithm. We will train the neural network policy we built earlier so that it\n",
        "learns to balance the pole on the cart. First, we need a function that will play one step. We will pretend for now that\n",
        "whatever action it takes is the right one so that we can compute the loss and its gradients. These gradients will just\n",
        "be saved for a while, and we will modify them later depending on how good or bad the action turned out to be:"
      ],
      "metadata": {
        "id": "fXcKFmJUK4XF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def play_one_step(env, obs, model, loss_fn):\n",
        "    with tf.GradientTape() as tape:\n",
        "        left_proba = model(obs[np.newaxis])\n",
        "        action = (tf.random.uniform([1, 1]) > left_proba)\n",
        "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n",
        "        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    obs, reward, done, truncated, info = env.step(int(action))\n",
        "    return obs, reward, done, truncated, grads"
      ],
      "metadata": {
        "id": "mQsNHe_LKN_m"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Walk through of the function:\n",
        "- Within the GradientTape block, we start by calling the model, giving it a single\n",
        "observation. We reshape the observation so it becomes a batch containing a single instance, as the model\n",
        "expects a batch. This outputs the probability of going left.\n",
        "- Next, we sample a random float between 0 and 1, and we check whether it is greater than `left_proba`. The\n",
        "action will be False with probability `left_proba`, or True with probability **1 – `left_proba`**. Once we cast\n",
        "this Boolean to an integer, the action will be 0 (left) or 1 (right) with the appropriate probabilities.\n",
        "- We now define the target probability of going left: it is 1 minus the action (cast to a float). If the action is 0\n",
        "(left), then the target probability of going left will be 1. If the action is 1 (right), then the target probability\n",
        "will be 0.\n",
        "- Then we compute the loss using the given loss function, and we use the tape to compute the gradient of the\n",
        "loss with regard to the model’s trainable variables. Again, these gradients will be tweaked later, before we apply them, depending on how good or bad the action turned out to be.\n",
        "- Finally, we play the selected action, and we return the new observation, the reward, whether the episode is\n",
        "ended or not, whether it is truncated or not, and of course the gradients that we just computed."
      ],
      "metadata": {
        "id": "bkjbOKhtLBGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let’s create another function that will rely on the `play_one_step()` function to play multiple episodes,\n",
        "returning all the rewards and gradients for each episode and each step:"
      ],
      "metadata": {
        "id": "af98AmQuLb6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
        "    all_rewards = []\n",
        "    all_grads = []\n",
        "    for episode in range(n_episodes):\n",
        "        current_rewards = []\n",
        "        current_grads = []\n",
        "        obs, info = env.reset()\n",
        "        for step in range(n_max_steps):\n",
        "            obs, reward, done, truncated, grads = play_one_step(\n",
        "                env, obs, model, loss_fn)\n",
        "            current_rewards.append(reward)\n",
        "            current_grads.append(grads)\n",
        "            if done or truncated:\n",
        "                break\n",
        "\n",
        "        all_rewards.append(current_rewards)\n",
        "        all_grads.append(current_grads)\n",
        "\n",
        "    return all_rewards, all_grads"
      ],
      "metadata": {
        "id": "wUcgEUr7K7I9"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code returns a list of reward lists: one reward list per episode, containing one reward per step. It also returns a\n",
        "list of gradient lists: one gradient list per episode, each containing one tuple of gradients per step and each tuple\n",
        "containing one gradient tensor per trainable variable"
      ],
      "metadata": {
        "id": "e5lWpEMWLl3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The algorithm will use the `play_multiple_episodes() `function to play the game several times (e.g., 10 times),\n",
        "then it will go back and look at all the rewards, discount them, and normalize them. To do that, we need a couple\n",
        "more functions:\n",
        "- the first will compute the sum of future discounted rewards at each step.\n",
        "- the second will\n",
        "normalize all these discounted rewards (i.e., the returns) across many episodes by subtracting the mean and\n",
        "dividing by the standard deviation"
      ],
      "metadata": {
        "id": "216ivKsMLqZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def discount_rewards(rewards, discount_factor):\n",
        "    discounted = np.array(rewards)\n",
        "    for step in range(len(rewards) - 2, -1, -1):\n",
        "        discounted[step] += discounted[step + 1] * discount_factor\n",
        "    return discounted\n",
        "\n",
        "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
        "    all_discounted_rewards = [discount_rewards(rewards, discount_factor)\n",
        "                              for rewards in all_rewards]\n",
        "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
        "    reward_mean = flat_rewards.mean()\n",
        "    reward_std = flat_rewards.std()\n",
        "    return [(discounted_rewards - reward_mean) / reward_std\n",
        "            for discounted_rewards in all_discounted_rewards]"
      ],
      "metadata": {
        "id": "EUGvuGl1LiE9"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if that works:"
      ],
      "metadata": {
        "id": "Q6swQGEbL7ER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "discount_rewards([10, 0, -50], discount_factor=0.8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qm5xLQhNLz5A",
        "outputId": "1383833d-efa6-4903-ebf2-28405deb8506"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-22, -40, -50])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Say there were 3 actions, and after each action there was a reward: first 10, then 0, then -50. If we use a discount factor of 80%, then the 3rd action will get -50 (full credit for the last reward), but the 2nd action will only get -40 (80% credit for the last reward), and the 1st action will get 80% of -40 (-32) plus full credit for the first reward (+10), which leads to a discounted reward of -22."
      ],
      "metadata": {
        "id": "zAvXaw_RL-o0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To normalize all discounted rewards across all episodes, we compute the mean and standard deviation of all the discounted rewards, and we subtract the mean from each discounted reward, and divide by the standard deviation:\n",
        "\n"
      ],
      "metadata": {
        "id": "y6Hij-1gMLz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "discount_and_normalize_rewards([[10, 0, -50], [10, 20]],\n",
        "                               discount_factor=0.8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1tmxeCUL9Te",
        "outputId": "fbfbfdbf-a9b1-4cac-ca5b-dabe3feb8ecd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([-0.28435071, -0.86597718, -1.18910299]),\n",
              " array([1.26665318, 1.0727777 ])]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let’s define the hyperparameters. We will run 150 training\n",
        "iterations, playing 10 episodes per iteration, and each episode will last at most 200 steps. We will use a discount\n",
        "factor of 0.95:"
      ],
      "metadata": {
        "id": "Gg6Uw6LoMSuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_iterations = 150\n",
        "n_episodes_per_update = 10\n",
        "n_max_steps = 200\n",
        "discount_factor = 0.95"
      ],
      "metadata": {
        "id": "iB9LZhFRMNmz"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create the neural net and reset the environment, for reproducibility:\n"
      ],
      "metadata": {
        "id": "WxSdtmMFMfcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(5, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "])\n",
        "\n",
        "obs, info = env.reset(seed=42)"
      ],
      "metadata": {
        "id": "Nr7GltHVMkie"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need an optimizer and the loss function. A regular Nadam optimizer with learning rate 0.01 will do just\n",
        "fine, and we will use the binary cross-entropy loss function because we are training a binary classifier (there are\n",
        "two possible actions—left or right):"
      ],
      "metadata": {
        "id": "RqF_OIzrMYwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.01)\n",
        "loss_fn = tf.keras.losses.binary_crossentropy"
      ],
      "metadata": {
        "id": "8LPLvMh5MU-J"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to build and run the training loop:"
      ],
      "metadata": {
        "id": "lSiDIgl6Mph7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for iteration in range(n_iterations):\n",
        "    all_rewards, all_grads = play_multiple_episodes(\n",
        "        env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
        "\n",
        "    # Displays some debug info during training\n",
        "    total_rewards = sum(map(sum, all_rewards))\n",
        "    print(f\"\\rIteration: {iteration + 1}/{n_iterations},\"\n",
        "          f\" mean rewards: {total_rewards / n_episodes_per_update:.1f}\", end=\"\")\n",
        "\n",
        "    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n",
        "                                                       discount_factor)\n",
        "    all_mean_grads = []\n",
        "    for var_index in range(len(model.trainable_variables)):\n",
        "        mean_grads = tf.reduce_mean(\n",
        "            [final_reward * all_grads[episode_index][step][var_index]\n",
        "             for episode_index, final_rewards in enumerate(all_final_rewards)\n",
        "                 for step, final_reward in enumerate(final_rewards)], axis=0)\n",
        "        all_mean_grads.append(mean_grads)\n",
        "\n",
        "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nFGhWhmMn-t",
        "outputId": "7f1b205a-2e82-43a8-a3b8-1674205b4c39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 132/150, mean rewards: 195.2"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the animation of one episode:"
      ],
      "metadata": {
        "id": "gT101l1uVOA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "show_one_episode(pg_policy)"
      ],
      "metadata": {
        "id": "KDyAcGDbVQaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-Learning\n",
        "Q-learning works by watching\n",
        "an agent play (e.g., randomly) and gradually improving its estimates of the Q-values. Once it has accurate Q-value\n",
        "estimates (or close enough), then the optimal policy is just choosing the action that has the highest Q-value (i.e.,\n",
        "the greedy policy).\n",
        "<br>**Q-Learning Algorithm:**<br>\n",
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAeMAAABdCAYAAACWwA6JAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAABaJSURBVHhe7d1riBvV+wfwZ3//SikIKrXoi9o9QfSN0Be14gVlJy+K4LUFFYTiJkJBCt7q5YUIk7xRFFQsVFCRJIhWoVJEQVEwWdu1ti8UVARXa6YXqrS2tXa3tFqZ/wtzhmeenUnmciaTy/cDi3XOJJmcOed55szlZMJ1XZcAAAAgN/+TCwAAAKC/kIwBAAByhmQMAACQMyRjAACAnCEZAwAA5AzJGAAAIGdIxgAAADlDMgYAAMgZkjEAAEDOkIwBAAByhmQMAACQs7FIxt9//z2dO3dOLh4658+fp9nZWTp16pQsAgAw4vvvv0eMYf7555++1MnIJ+NyuUx33XUXLV26VBYNnSVLltDGjRtp/fr15DiOLAYASEXHy4suukgWja0LLriA7rrrLnrllVcyjbsjnYzL5TK1Wi2q1WqyaGi1221yHIfK5XKmDQMAxku5XKZ6vT5S8dKUZrNJjUaD6vV6ZnF3ZJNxpVLxGpZlWbJ4qDWbTSRkADBGx8tmszly8dIEpVTmCfn/KpVKRS4cdq1Wi8rlMtm2TaVSSRYPvYsvvpj+/PNPajQadPHFF6PzAEBiOl6WSiV67LHHZDF06LhbrVYzibsTruu6cuEwcxyHCoUCKaWo3W7L4pFSKBTIcRxqt9uklJLFAABdjVO8NCWruDtyp6nL5TIREdm2LYtGTrPZJKUUFYvFTE6bAMBo0/FyenpaFkGIWq2WSdwdqWRcqVSo1WqRZVkjeXpaUkrR9PQ0OY5D9XpdFgMAhOLxcgSvVmbGsqxM4u5InaaemJgg6txxbPL0waDL6rQJAIwuHS9rtdpYDF5MchzHGxmbirsjMzLWR3ZKKSMVM0z0jQTValUWAQAswuMlEnF8Sikv7poaHY9EMnYcx0tE43jtQ3/nrG65B4DRMs7x0hRdd9Vq1UjcHYlkrI9MlFJjee3DsizvbABGxwDQDR8Vj2O8NIXHXROj45FIxo1Gg2jMj/L03eMYHQNAN4iX5pgcHQ99MnYcx6sE0w9hD5NSqeQdpbVaLVkMAIB4aRivw7Rxd+iTMT89gMb1H33kCwDAIV6axU9Vp427fXu06eDBg/Tuu+/S7OwsHTx40Ft+88030/33308333yzb/2o9O35CjPIUKVS8a4Zm7rdHgD6D/FyeOgf2KCUcTfzkXGr1aJCoUCTk5P0+uuv0/z8PG3YsIEef/xxWrNmDb322mt0yy23eM/KxsHXx/UPs6dMAKD/EC+Hz9TUlPfvVHHXzUi73XZt23aJyCUi17ZtuYrrdtZTSrlE5Cql3GazKVcJxd8/zutGma5Ly7JkEQAMqDjx0rIsxMsBo+u1VCrJosgyScbNZtPbOKWU22635So+7Xbbt35UulFSdscUQ0cnY9QJwHBAvBx+JgZBxk9Tt1otKhaLRJ3TplHOoSulvB+0djq/0xuFPiXQ6/3HCT9VHfc0FiTXarXowQcflIsBuhrHeHn27Fm5aODE3Ub+JEvSuGs0GfOGRZ05T6PiDSTKeXd+U0PejWtQRalHMMNxHGo2m3IxQKg08TLu/SGHDh2Si/rOcRyamJigZcuWUbFYpFOnTslVclev171tjDMhCs9BA5GM+exP+uf9opJf5pdffvGVSzwZ8wvo447XxczMjK/MtGPHjtEHH3xAx44dk0UA0EOaeMk5jkM///yzXOzDk3FejzTxx6pardZAHCBI/CxDtVql8+fP+8rD8LibezLWP8dFnQko4u5wFfMHHnbv3i0XQZ/PEmzbto2efPJJWrFihSwCgC5MxEv+muPHj/vKpdnZWe/fk5OTvrJ+kYODiy66yPf/eZNJVClFS5Ys8S2LQn7PqIwkY4f9UAMZum1+//79clEoUwno9OnTdPr0aZqbm6MjR47I4szMz8/H+r7dyDMMWWo0Gkb2tUlHjhwJ3Hd57Ncs6Layf/9+mp+fl8W5WFhY8PrOkSNHaG5ujk6fPu2V67qfm5vzvY7jr426j44fP04ff/yx72/Pnj1ytYFjKl7y/n3ixAlfWTd5xEvHcXyn05VSdMUVV/jWMSlJTJXzS8fZL0birryjK4lSqeTdpZfmbjJ+J/C+fftksc8dd9zhrWviNn19279SKvGjVnHxRw30Z9q27T3mEPZ4Qy/8PbOi7+gcJJZl+fZdrVZzXbFvs6qXWq2Wyftq7Xbb18/09+jWRkqlkvvMM8/IxcZNTU35tos6fZI/tii3W98xLB/VCVonCN+npVLJVzfd6mX9+vWLtkf/6dfwfin/TJDbmhSvrz179shin0ceecRb10RMixsv+R3j1OXRrbTSxFS537u1P4nf4Z40BxppXfwL6ACYBH+ffiZjHQz4tuvgp5Ryz5w541s/LR6A+HNpfIfqvyR4J82KZVldG3a/2bbtdQLd8XWg4EFCB8LPP/9cvEM6WSbjoHbB/4L2g37N3r17ZZFxO3bs8PoK3ya5bTJg6f0Utk5YfdZqNd/7aDrgdquX7du3u7ZtBx7Y6DYiE4dO7kHvlwSvJ1Pxslcy3rBhg7du1vFyYWHBt74bkOhMixpTw9qUK+qz23ph0rzWNZGMt2/f7vsScY4mON7BiMg9fvy4XMVn9erVqT/TZcFZdgreIU0nY91ogo6geD0kfYCcd/a///5bFqemO12aejdNsZGwbEt6O/nyV155RbxDOlkl47aYFEcnBf7MaNC+KJVKge0rSzLgBgV9uY7sd3KdoPfg+zGoznWf5gk2SLe+pt/DVALW3n//fd/3l/stKtnG//jjD7mKz9q1a1N/psv2jawXHi+DkrFsr6aljakyacvvFwV/fRLJXsU8/PDDqTfCFaduoryPqWSsA50MCnx7TOKNMghv1EkahJtxMtaJuFuQ67fdu3f76pPvO76dvO6/++47b7kJWSXjbklBj0aCyuV3j+rIkSNyUWQ86AUFRTdCInUj9AE9CuIHYFyv13NBiT9oxG7K448/7n0ehcSAKOLGS1PJWMcWWTe8HiWZ6IL2WRq9Yipvc2Gfzbc/rF32wr9jEslexaxZs8bbgLCjjih4AonyPqaSMd8BPHi12+3QHZdUlCM0vk6SYOpmlIybzabX6JNuV1beeustX33y78/pfZqmvYTJIhmfPHnSpS4BxBWJSdMHTHE9+uijLhG5DzzwgCyKJG77DlsnTjINEuf1uv6InWGwLCv0YCKt66+/vuf3jyJuvDSVjPV7xImXfH8kaZfdRGlPfJ2w786Tca82EyYs7kSV7FUMT8ZJvwSvrG4VxplKxrwCqdNYLMsKbVhp8CO4sIRmYkRuOhnzbdL1k+Xftm3b5CbEorc1rHPGMTc3t2j7gv50ncvlYX9ROI4TaV0d7HSboh4JPMiHH37o6wdbt26Vq/TE+3FYLIiSKKOsw+lEYHeuBfP2H/X1/LubThgcT8ZRti2IjJdhsYQzlYwtFsN0XVk97h8xkejC8O0Jq4coMZV/p6T1w9vdv//+K4t7MvJoU1r8uaxSqWTs1vso5O3r+hb8crkcawaWKPit/WHPFfJnDwfF9PS0b59MTU1l+nfdddf5Pj8O/njClIHJYJYvX75o+4L+9O+ayuVBf7LNhZmcnIw0q5feN47jeN8/bvuRj5lcddVVvv+Pqx99uFKpULFYpEKhQOVymRqNBjmOE9q3wig2vSR16ZuDQsbLfm7vlOhTOl5Wq9XQeMm3N2ydpEzEVP4oUtz5LsL8738JUqvMznHdfffdqY56+FFwnKMSUyNjt3OkKY/49F/Y0VZc+/bt896z24gtTV1qpkfGmm3brury2Mgg4EfhadtFHFmcpo5Dj056jVK62blzp3vTTTe5b7zxhiyKhI/YwkbmUUa9vdbhp5Yp4Pp0r9cHkadSTfV76Z577om9bZwcxUdt46ZGxm7MeMm313T/+Prrr733DoupfL+GtUlTI3deD0kkSN/hksw8wh9+r9VqkY9KVq1aJRfFpo+ISqUSNZtNarfbi6alizLvaxQnT570/h02Aw4f1YUd5eWpUqmQbdvUaDSMH+Gawttg1LY0CpRS1Gg0qNVqhY4Aelm/fj3Nzs7Spk2bZNFAKZfLvtFOlLMHvZTLZe/shtP58YXEkzdElCRe8uka48RLeeYjibB42W2ebF6Htm37ytLiUybLEbvGP79XTFVK5RrXUifjp556Si7y6EY9MTFBxWJx0QwnfEo427YTB5EkCoWCd3pLU50p5njnjtrY4wh7T945dcOp1+u0ceNGtlZvvAFecMEFvrK0SqUSTU9PU7VaXdTxBkGvU1KjzHGcvl/m6bfDhw97+9iyrMg/rlCpVOjbb7+Vi4mIvB9raDabvl9D4gMFU7Zs2eL9WyZ7Hi8LhcKieFmv1wcyXvJ9INue3l6llPHtveSSS+SiRYIOzuv1Oj377LPecr2fo14+ykrqZLxq1SrvS+rrB/rfhUKBJicnqd1u09TUlK9x1+t17/9LpVLsI5I0I+Mvv/xyUUfgeIMKO5ravHkzrV69mp5//nlZFOjKK6/0/i0brBaUSBqNBq1cuZKtlT89Qs4iWKVh+nrxMNFnW0b9e//1119y0SJBfXtmZob+/PNPudgbEOiEYlmWN4Kr1+ux41IvK1euDBxJynipD3i1er3uJcIk8ZKPjIPqp5vdu3d3fU23eKmTYa9Ed+LEidgxdfny5d6/g2KqE3IPRbVapbVr1xKJ9pTmYIHXT9C2RCLPWyfBrxXp6wKlzhR1nNWZdcfEOfrNmzd77yGvU/SyY8eOrq/V30duv/bJJ5/4rg98+umncpVFjh8/3vUz+TUY/rlE5B4+fNi3bjdZXqORVMBkE3nK63qxOwDXjPV3z1OUu6mjrNPtmu+hQ4d6tu+gvqSUcr/44gvferrO5DVnfk06i+vH8vq02yVe6rvEw+ojqjTTYe7cubPra/U+lfXosuuovXz88cfeukTkfvbZZ3KVRZLEVB0fdUz94YcfXOrSlqIyEXd711JEvMFYnUc9ZMPh66Rt5DwZh12YD3P06FGXOg2bb0ObTafXrUL37t3rfTYRuS+88IJcJVDQe9udm6KsziMvuv7cTmMKauDd8EYR97VxBQWQPEV5hCEreSfjPPfFq6++6rVj3r9LpZIXA+zOzGFp13FFkOV9WCdRy7K8tqDjEBG5v/76q/viiy/6Xq/X4TGEHzDIbdi/f7+3Xhr8M8LipRzkpImXL774ovdecePlsWPHFtW1GzFe/vTTT+5XX30lFy8ik/FLL70kVwkU9PlxYuqZM2fcjz76yP3pp5+8ZUmYiLtGo5a8y1E3Ytn4ZaPT4oxm3nnnHe/9kgQhefTLtzls+7hS53lGpZS7ZcsWWRyKNx75eW0xsb5KMPLkR91J6iUOHSzibmNW9P6Msv9MyzsZq5DZqPpBJ5OgP+ocGMnlSddxRRKQf7wv8f6tk8i1114b+N68zfBELtczOad5W/z4h8owXr733nve+yWJC7I++TaHbV9cPKY+/fTTsjhU1jE1iihnfHoxmoy1duchfN2w7M4Rr254QZVh23asYLJr165UjUtbWFhwjx496h49ejRwTtVebNuOlYzdzmfqzw2ysLDgOo4jF0fCg1TSRhHH/Py8XJSb+fl59/fff5eL+yLPZKwDQZz+Mwr0/t6/f39u+92EsHip+3LQiFiXRzU7O+vFhaQjN9dAvIzCtu3II2Mty5gahYm4m0kyDqM3WG6sHs0FJelu9BFPmsaVlmVZbr1el4tzwxtFUCeGbOSZjLsd5MLw0n1ZDjaGOV5GYVmW+/bbb8vFA42fzUgad1PfTZ1Eo9HwZrgqFotULBZTPZKR1yM2+u7xW2+9VRblJujxKMje7bffTi+88IJc3Bf88REYPS02I6CpeNnt7ug86Zi6bt06WTQ0ksbdXJKxvuWcP6va69b3IPxL59G46vU6KaXo8ssvl0W5QWDOx4oVK+i+++6Ti/vC6TxfDKMpi3g5qHRMveyyy2TRQDMRd/uajMMaQ61WCy3rhj9T2e9k3OrMxxp10oF+4HWQpLPC8NHPUY7688XjKCwm2rYdWtYNbyN5nU3sZhBjahSm4m5fk3HQLCxpZpLhRyH9blzVajVxp8hKvw9IIH96nw9SOwQzlFKLppC0bTv2hB8aj5eDGCvK5fLAxdQoTE1jPOH+92B235w5c8b776WXXiqLYysUCuR0fqnFxBy1Uf3xxx9Gtt+kSqXizdrT590KOanX69RoNPra9gfd1q1bacWKFXTnnXfShRdeKIuHyqjEyygGMaZGUSwWqdVqkVKK2u22LI6s78nYNJ6A2u12qnP2w25iYoKoc3Q2aB0NoB+czrSSRET79u1L9XOcowjx0jwdd9OctaB+n6bOAj/F3e9T1YPE1HULgGEmf1wB/Phd2Kir9HjOSXOKmkYhGSv2Y9CNRkMWjw3dsYKuy/fTzMwMbdq0ia6++mpat24drVu3jg4cOCBXA4hs165d9NBDD9E111xDt912Gx04cIB27doV2K70o31KKYyKA/B4meQnHMGP30WdNhn3ddKPrPCpyOI+CD8q9AP9ckKVftKTFPCZoOSEFO12233iiSfYqwDC6b6tJ1JoNpteW5eTYbTZ/MB59oNBh3hpjsn2NhLJ2B2QZJQXPSNPXjNAuSwRy9lndIDUs/7Ytu2+/PLLvnUAguh2Lfu0PsCTy/m87Egy3Y1zvDRFxzxTcXdkkvE4H+3l3bF6HQzo7dOJGSAK3Z8lPfWg7Oemg+Mow4FLeqbj7tBfM9bG9cYEPbWdUirVnXxp6Lsze904Vq1W019XgbGwZ88eopAZjfRjJLIs6g/Zw383G41jvDSlUqmYj7syOw8zfj1pXI72TB+dxXX27NnQEYymRzKDPkE9DI4tW7YEtmt9dkVeL3Y7I2mMiqNrs58XHJd4aYquN5O/lDYyI2PqHO3po+JxONrTo2LLsswdncV07tw5opARjIQpGyEqPTKWZ1J0v56cnCRij/T99ttvRBgVx6KUGqt4aYoeFVuWZfbJFZmdR8E4HO3pEcIgjAR6jXr1/pA3dwGEuffeewP7MG9LemTnuq775ptvBq4PvY1DvDQly7g7UiNjrdlsklKKisXiQM7BakK5XCbq/MhG3pRS3iida7VaVCwWvf93HIccx/FGPQBhbrjhBiIxmU25XPbNxV2v172R3TfffBN4HRl6G4d4aUqWcXfop8MM0+r8Buj09HRup3Czoqe0q9VqZk+TJOQ4DhWLRVJKeaeiZ2ZmvNNguqNblkWO49Bzzz1HN954o3wbAJ9CoeC1qZmZGa9t6R9paTQavrmAz507R0uXLmXvAFHxeJnmt5JHmY67zWZz0eUTI+RQeZTUajVXKbXoJpBhph9JMHnjgCkHDx50Z2dn3R9++ME9deqULHYPHjwYuBwgzOHDh90ff/zR1270MjBL3wA7SvHSlH7E3ZEdGWuVSoUajQbNzMzQqlWrZPFQOXv2LC1btmxgRsQAMFr0CFmfugaihYUFuvDCCzOPuyOfjImIzp8/T0uWLJGLh9IofRcAGDyIMYv1o07GIhkDAAAMspG8mxoAAGCYIBkDAADkDMkYAAAgZ0jGAAAAOUMyBgAAyBmSMQAAQM6QjAEAAHKGZAwAAJAzJGMAAICcIRkDAADkDMkYAAAgZ0jGAAAAOUMyBgAAyBmSMQAAQM6QjAEAAHKGZAwAAJAzJGMAAICcIRkDAADkDMkYAAAgZ0jGAAAAOft/+fvYndiMWHcAAAAASUVORK5CYII=)"
      ],
      "metadata": {
        "id": "OkY6gNmBM3sw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s implement the Q-learning algorithm. First, we will need to make an agent explore the environment. For this,\n",
        "we need a step function so that the agent can execute one action and get the resulting state and reward:"
      ],
      "metadata": {
        "id": "QEGcY9JmP5s_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def step(state, action):\n",
        "    probas = transition_probabilities[state][action]\n",
        "    next_state = np.random.choice([0, 1, 2], p=probas)\n",
        "    reward = rewards[state][action][next_state]\n",
        "    return next_state, reward"
      ],
      "metadata": {
        "id": "z7EqGz8mMt7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let’s implement the agent’s exploration policy. Since the state space is pretty small, a simple random policy\n",
        "will be sufficient. If we run the algorithm for long enough, the agent will visit every state many times, and it will\n",
        "also try every possible action many times:"
      ],
      "metadata": {
        "id": "z4HIo9x2QAXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def exploration_policy(state):\n",
        "    return np.random.choice(possible_actions[state])"
      ],
      "metadata": {
        "id": "zOh1aZeSP8NK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the Q-values:"
      ],
      "metadata": {
        "id": "osgI_Em_QQEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "possible_actions = [[0, 1, 2], [0, 2], [1]]\n",
        "\n",
        "Q_values = np.full((3, 3), -np.inf)\n",
        "for state, actions in enumerate(possible_actions):\n",
        "    Q_values[state][actions] = 0"
      ],
      "metadata": {
        "id": "gBkUIAy5QS1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we are ready to run the Q-learning algorithm with learning\n",
        "rate decay (using power scheduling):"
      ],
      "metadata": {
        "id": "dkjFHgtaQFR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha0 = 0.05  # initial learning rate\n",
        "decay = 0.005  # learning rate decay\n",
        "gamma = 0.90  # discount factor\n",
        "state = 0  # initial state\n",
        "history2 = []  # extra code – needed for the figure below\n",
        "\n",
        "transition_probabilities = [  # shape=[s, a, s']\n",
        "    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
        "    [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
        "    [None, [0.8, 0.1, 0.1], None]\n",
        "]\n",
        "\n",
        "for iteration in range(10_000):\n",
        "    history2.append(Q_values.copy())  # extra code\n",
        "    action = exploration_policy(state)\n",
        "    next_state, reward = step(state, action)\n",
        "    next_value = Q_values[next_state].max()  # greedy policy at the next step\n",
        "    alpha = alpha0 / (1 + iteration * decay)\n",
        "    Q_values[state, action] *= 1 - alpha\n",
        "    Q_values[state, action] += alpha * (reward + gamma * next_value)\n",
        "    state = next_state\n",
        "\n",
        "history2 = np.array(history2)"
      ],
      "metadata": {
        "id": "ubWmp9FxQCf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This algorithm will converge to the optimal Q-values, but it will take many iterations, and possibly quite a lot of\n",
        "hyperparameter tuning."
      ],
      "metadata": {
        "id": "5KaW8fzFQiZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show an animation of the trained DQN playing one episode:"
      ],
      "metadata": {
        "id": "WPnjmqAbVcSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_one_episode(epsilon_greedy_policy)"
      ],
      "metadata": {
        "id": "28_oLEJ3Vgmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Deep Q-Learning\n",
        "To reduce the algorithm’s sensitivity to large error, we need to implement a Deep Q-Network (DQN). To solve the CartPole\n",
        "environment, we do not need a very complicated neural net; a couple of hidden layers will do:"
      ],
      "metadata": {
        "id": "woazaA-TQu9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "input_shape = [4]  # == env.observation_space.shape\n",
        "n_outputs = 2  # == env.action_space.n\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape),\n",
        "    tf.keras.layers.Dense(32, activation=\"elu\"),\n",
        "    tf.keras.layers.Dense(n_outputs)\n",
        "])"
      ],
      "metadata": {
        "id": "KGbXktvGQeiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To select an action using this DQN, we pick the action with the largest predicted Q-value. To ensure that the agent\n",
        "explores the environment, we will use an ε-greedy policy (i.e., we will choose a random action with probability ε):"
      ],
      "metadata": {
        "id": "a1-23qZnROP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy_policy(state, epsilon=0):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_outputs)  # random action\n",
        "    else:\n",
        "        Q_values = model.predict(state[np.newaxis], verbose=0)[0]\n",
        "        return Q_values.argmax()  # optimal action according to the DQN"
      ],
      "metadata": {
        "id": "sp1vTNiIRHa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of training the DQN based only on the latest experiences, we will store all experiences in a *replay buffer\n",
        "(or replay memory)*, it will contain the agent's experiences, in the form of tuples: `(obs, action, reward, next_obs, done)`, and we will sample a random training batch from it at each training iteration. This helps\n",
        "reduce the correlations between the experiences in a training batch, which tremendously helps training. For this,\n",
        "we will just use a double-ended queue (`deque`):\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x_psdV5xRbXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "\n",
        "replay_buffer = deque(maxlen=2000)"
      ],
      "metadata": {
        "id": "ty_DXnTfRYe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will need a small function to sample a\n",
        "random batch of experiences from the replay buffer. It will return 6 NumPy arrays corresponding to the 6\n",
        "experience elements:"
      ],
      "metadata": {
        "id": "TGkuFS1KR_yS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_experiences(batch_size):\n",
        "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
        "    batch = [replay_buffer[index] for index in indices]\n",
        "    return [\n",
        "        np.array([experience[field_index] for experience in batch])\n",
        "        for field_index in range(6)\n",
        "    ]  # [states, actions, rewards, next_states, dones, truncateds]"
      ],
      "metadata": {
        "id": "pQee3Zp3Rkzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s also create a function that will play a single step using the ε-greedy policy, then store the resulting experience\n",
        "in the replay buffer:"
      ],
      "metadata": {
        "id": "dVFiq0moSIOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def play_one_step(env, state, epsilon):\n",
        "    action = epsilon_greedy_policy(state, epsilon)\n",
        "    next_state, reward, done, truncated, info = env.step(action)\n",
        "    replay_buffer.append((state, action, reward, next_state, done, truncated))\n",
        "    return next_state, reward, done, truncated, info"
      ],
      "metadata": {
        "id": "s4m3K5v5SGuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let’s create one last function that will sample a batch of experiences from the replay buffer and train the\n",
        "DQN by performing a single gradient descent step on this batch:"
      ],
      "metadata": {
        "id": "FejpUZL6SLmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For reproducibility, and to generate the next figure\n",
        "env.reset(seed=42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "rewards = []\n",
        "best_score = 0"
      ],
      "metadata": {
        "id": "FW0oy1cfSKly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "discount_factor = 0.95\n",
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-2)\n",
        "loss_fn = tf.keras.losses.mean_squared_error\n",
        "\n",
        "def training_step(batch_size):\n",
        "    experiences = sample_experiences(batch_size)\n",
        "    states, actions, rewards, next_states, dones, truncateds = experiences\n",
        "    next_Q_values = model.predict(next_states, verbose=0)\n",
        "    max_next_Q_values = next_Q_values.max(axis=1)\n",
        "    runs = 1.0 - (dones | truncateds)  # episode is not done or truncated\n",
        "    target_Q_values = rewards + runs * discount_factor * max_next_Q_values\n",
        "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
        "    mask = tf.one_hot(actions, n_outputs)\n",
        "    with tf.GradientTape() as tape:\n",
        "        all_Q_values = model(states)\n",
        "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
        "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
      ],
      "metadata": {
        "id": "3aw41ihySTQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can train the model:"
      ],
      "metadata": {
        "id": "DeG8wHMtSXx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(600):\n",
        "    obs, info = env.reset()\n",
        "    for step in range(200):\n",
        "        epsilon = max(1 - episode / 500, 0.01)\n",
        "        obs, reward, done, truncated, info = play_one_step(env, obs, epsilon)\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "    # Displays debug info, stores data for the next figure, and keeps track of the best model weights so far\n",
        "    print(f\"\\rEpisode: {episode + 1}, Steps: {step + 1}, eps: {epsilon:.3f}\",\n",
        "          end=\"\")\n",
        "    rewards.append(step)\n",
        "    if step >= best_score:\n",
        "        best_weights = model.get_weights()\n",
        "        best_score = step\n",
        "\n",
        "    if episode > 50:\n",
        "        training_step(batch_size)\n",
        "\n",
        "model.set_weights(best_weights)  # Restores the best model weights"
      ],
      "metadata": {
        "id": "USZxnesTSVzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show an animation of the trained DQN playing one episode:"
      ],
      "metadata": {
        "id": "vmq1HAMjVp7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_one_episode(epsilon_greedy_policy)"
      ],
      "metadata": {
        "id": "FmkOwI-USfPl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}