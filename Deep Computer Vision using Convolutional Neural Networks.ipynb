{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxlM3kRUQWLr/YroaH36yM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinconggg/Machine-Learning/blob/main/Deep%20Computer%20Vision%20using%20Convolutional%20Neural%20Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional Layers\n",
        "Convolutional layers are the core building blocks of Convolutional Neural Networks (CNNs). They apply convolution operations to the input data to extract features such as edges, textures, and patterns.\n",
        "\n",
        "In a convolutional layer:\n",
        "- A **filter (kernel)** slides over the input, performing element-wise multiplication and summation to produce a **feature map**.\n",
        "- **Strides** control how much the filter moves at each step.\n",
        "- **Padding** can be applied to preserve the spatial dimensions of the input.\n",
        "- **Activation functions** like ReLU are typically applied to introduce non-linearity.\n",
        "Convolutional layers help CNNs learn **spatial hierarchies** in data, making them highly effective for tasks like **image classification, object detection, and segmentation**."
      ],
      "metadata": {
        "id": "COqpP7VGSVfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing Convolutional Layers with Keras\n",
        "First, load and preprocess a couple of sample images, using Scikit-Learn's `load_sample_image` function and Keras's `CenterCrop` and `Rescaling` layers:"
      ],
      "metadata": {
        "id": "qoQ_1MYOSz8h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kLjytW5JQ8V2"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_sample_images\n",
        "import tensorflow as tf\n",
        "\n",
        "images = load_sample_images()[\"images\"]\n",
        "images = tf.keras.layers.CenterCrop(height=70, width=120)(images)\n",
        "images = tf.keras.layers.Rescaling(scale=1/225)(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the shape of the \"images\" tensor:"
      ],
      "metadata": {
        "id": "C84lvkMHVj7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNUjcJfUVSYf",
        "outputId": "b4cef4c6-f6f5-4efe-a7f2-64898bded25c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([2, 70, 120, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is a 4D tensor: the 2 sample images explains the first dimension. Each image is 70x120, since that's the size specified in the `CenterCrop` layer, which explains the second and third dimension. Lastly, each pixel holds one value per color channel, and there are 3 of them - red, green and blue, which explains the last dimension."
      ],
      "metadata": {
        "id": "t1Qfr_XjVrD1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's create a 2D convolutional layer and feed these images to see what comes out. To do so, Keras provides a `Convulution2D` layer, alias `Conv2D`. Create a convolutional layer with 32 filters, each of size 7x7 (``using `kernel_size=7`), and apply this layer to 2 of the images:"
      ],
      "metadata": {
        "id": "6RgpmofNWUZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=7)\n",
        "fmaps = conv_layer(images)"
      ],
      "metadata": {
        "id": "c_0zK5txVoo1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at the ouput's shape:"
      ],
      "metadata": {
        "id": "HpV9JV4ZW6GC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fmaps.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAjRwaFqW3g1",
        "outputId": "b5a8e26d-1950-45d6-89e9-b49fbb2a5e10"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([2, 64, 114, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output's shape is similar to the input's shape, but with 2 main differences. **First**, there are 32 channels instead of 3. This is because we set `filters=32`, so we get 32 output feature maps; instead of the intensity of red, blue and green at each location, we now have the intensity of each feature at each location. **Second**, the height and width have both shrunk by 6 pixels. This is due to the fact that the `Conv2D` layer does not use any zero-padding by default, meaning that we lose a few pixels on the sides of the output feature maps. In this case, since the kernel size is 7, we lose 6 pixels horizontally and vertically. (3 pixels on each side)"
      ],
      "metadata": {
        "id": "sTmOTqNzXQiz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, if we set `padding=same`, then the inputs are padded with enough zero on all sides to ensure that the output feature maps end up with the same size as the input:"
      ],
      "metadata": {
        "id": "p2O_vqtDYAsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=7,\n",
        "                                    padding=\"same\")\n",
        "fmaps = conv_layer(images)"
      ],
      "metadata": {
        "id": "I7p9BNu_W8VY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fmaps.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZIHVGT_YLsV",
        "outputId": "6937fac8-b556-4b47-e6ef-f6bc18660196"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([2, 70, 120, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at the layer's weights. Just like a `Dense` layer, a `Conv2D` layer holds all the layer's weights, including the kernels and biases:"
      ],
      "metadata": {
        "id": "oNemolMrYrB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kernels, biases = conv_layer.get_weights()\n",
        "kernels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljtePL0gYNfz",
        "outputId": "df0e2c95-0f80-41b2-ffa8-8668219a4fcc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7, 7, 3, 32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "biases.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJOXLzYKY2Pb",
        "outputId": "2681eae6-d7e9-4b51-e69c-9d833340c58a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32,)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pooling Layers\n",
        "Pooling layers are used in Convolutional Neural Networks (CNNs) to **reduce the spatial dimensions of feature maps**, which helps to reduce computation, prevent overfitting, and make the network more efficient.\n",
        "\n",
        "There are two main types of pooling:\n",
        "- **Max Pooling:** Takes the maximum value from each region of the feature map, focusing on the most prominent features.\n",
        "- **Average Pooling:** Takes the average value from each region, smoothing the feature map.\n",
        "\n",
        "Pooling layers:\n",
        "- **Reduce dimensionality** by downsampling the input.\n",
        "- **Retain important features** while discarding less important details.\n",
        "- **Provide translation invariance**, making the network more robust to small shifts in the input.\n",
        "\n",
        "Pooling layers are typically applied after convolutional layers to **extract dominant features** and make the model more efficient without losing key information.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dxLZs-aKZAJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing Pooling Layers with Keras\n",
        "The following code creates a `MaxPooling2D` layer, alias `MaxPool2D`, using a 2x2 kernel. The strides default to the kernel size, so this layer uses a stride of 2 (horizontally and vertically). By default, it uses a \"valid\" padding (i.e., no padding at all):"
      ],
      "metadata": {
        "id": "ddA4YAXcaoHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_pool = tf.keras.layers.MaxPool2D(pool_size=2)"
      ],
      "metadata": {
        "id": "YXbslsiqY3BT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create an *Average Pooling* layer, just use `AveragePooling2D`, alias `AvgPool2D`, instead of `MaxPool2D`. However, most use max pooling layers rather than average pooling layers since they generally perform better."
      ],
      "metadata": {
        "id": "DCqWKTFObGW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that max pooling and average pooling can be performed along the depth dimension instead of spatial dimensions. This allows the CNN to learn to be invariant to various features. For example, it could learn multiple filters, each detecting a different rotation of the same pattern, and the *depthwise max pooling* layer would ensure taht the output is the same regardless of the rotation."
      ],
      "metadata": {
        "id": "GKtA-X0Abhe-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras does not include a *depthwise max pooling* layer, but we can implement a custom layer for that:"
      ],
      "metadata": {
        "id": "yODPwY_HcJ_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DepthPool(tf.keras.layers.Layer):\n",
        "    def __init__(self, pool_size=2, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.pool_size = pool_size\n",
        "\n",
        "    def call(self, inputs):\n",
        "        shape = tf.shape(inputs)  # shape[-1] is the number of channels\n",
        "        groups = shape[-1] // self.pool_size  # number of channel groups\n",
        "        new_shape = tf.concat([shape[:-1], [groups, self.pool_size]], axis=0)\n",
        "        return tf.reduce_max(tf.reshape(inputs, new_shape), axis=-1)"
      ],
      "metadata": {
        "id": "6jurmfAjbA6e"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This layer reshapes its inputs to split the channels into groups of the desired size (`pool_size`), then it uses `tf.reduce_max`  to compute the max of each group."
      ],
      "metadata": {
        "id": "rYvDX4pacV7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One last type of pooling layer that is often seen is the *global average pooling* layer. It works very differently: all it does is compute the mean of each entire feature map. This means that it just outputs a single number per feature map and per instance. Although it can be extremely destructive (most of the information in the feature map would be lost), but it can be useful just before the output layer. To create such a layer, simply use the `GlobalAveragePooling2D` class, alias `GlobalAvgPool2D`:"
      ],
      "metadata": {
        "id": "3yYucEUEdI0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "global_avg_pool = tf.keras.layers.GlobalAvgPool2D()"
      ],
      "metadata": {
        "id": "nyCwuPyKcTGg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, if we apply this layer to the input images, we get the mean intensity of the red, green and blue for each image:"
      ],
      "metadata": {
        "id": "-njlpLT4d-XT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "global_avg_pool(images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Es5b1O1ad7rD",
        "outputId": "31428bca-ea91-4f2f-dd8d-fbe200dfa0f4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
              "array([[0.72917455, 0.6768054 , 0.6601711 ],\n",
              "       [0.86481947, 0.29479   , 0.12295388]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN Architectures\n",
        "Typical CNN Architectures stack a few convolutional layers (each one generally followed by a ReLU layer), then a pooling layer, then another few convolutional layers, then another pooling layer, and so on. The image gets smaller and smaller as it progresses through the network."
      ],
      "metadata": {
        "id": "abTSQkM8eGE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code implements a basic CNN to tackle the Fashion MNIST:"
      ],
      "metadata": {
        "id": "ruFl0Y_6jcJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Import the Fashion MNIST dataset from TensorFlow\n",
        "mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
        "# Load the dataset into training and testing sets\n",
        "(X_train_full, y_train_full), (X_test, y_test) = mnist\n",
        "# Normalize and Scale to range [0-1]\n",
        "X_train_full = np.expand_dims(X_train_full, axis=-1).astype(np.float32) / 255\n",
        "X_test = np.expand_dims(X_test.astype(np.float32), axis=-1) / 255\n",
        "# Split the full training set into a smaller training set and a validation set\n",
        "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
        "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]"
      ],
      "metadata": {
        "id": "ZXveAZihkRuT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, padding=\"same\",\n",
        "                        activation=\"relu\", kernel_initializer=\"he_normal\")\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),\n",
        "    tf.keras.layers.MaxPool2D(),\n",
        "    DefaultConv2D(filters=128),\n",
        "    DefaultConv2D(filters=128),\n",
        "    tf.keras.layers.MaxPool2D(),\n",
        "    DefaultConv2D(filters=256),\n",
        "    DefaultConv2D(filters=256),\n",
        "    tf.keras.layers.MaxPool2D(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(units=128, activation=\"relu\",\n",
        "                          kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(units=64, activation=\"relu\",\n",
        "                          kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(units=10, activation=\"softmax\")\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kw63yZ3yeE-_",
        "outputId": "66f045ef-8516-4cf2-c675-4cc7b2728708"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of code:\n",
        "- `functools.partial` function was used to define `DefaultConv2D`, which acts just like `Conv2D` but with different default arguments: a kernel size of 3, \"same\" padding, the ReLU activation function, and its corresponding He intializer.\n",
        "-Next, we create the `Sequential` model. Its first layer is a `DefaultConv2D` with 64 filters. It sets `input_shape=[28, 28, 1]`, because the images are 28x28 pixels. Note: when loading the Fashion MNIST dataset, ensure that each image has this shape, else use `np.reshape` or `np.expanddims` to change the dimension.\n",
        "- We then add a max pooling layer that uses a default pool size of 2, so it divides each spatial dimension by a factor of 2.\n",
        "- The same structure is then repeated twice: 2 convolutional layers followed by a max pooling layer. Note: for larger images, we could repeat this structure several more times.\n",
        "- Note that the number of filters doubles as we climb up the CNN woards the output layer (from 64 to 128 then 256). It is a common practice to double the number of filters after each pooling layer.\n",
        "- Next is the fully connected network, composed of 2 hidden dense layers and a dense output layer. Since it's a classification task with 10 classes, the output layer has 10 units, and it uses the softmax activation function."
      ],
      "metadata": {
        "id": "zUTtgYthlyB1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN Architectures Summary\n",
        "**1) LeNet-5:**\n",
        "- **Designed for:** Handwritten digit recognition\n",
        "- **Architecture:** 7 layers (Convolution, Pooling, Fully Connected)\n",
        "- **Usage today:** Rarely used in modern applications\n",
        "\n",
        "**2) AlexNet:**\n",
        "- **Designed for:** ImageNet classification\n",
        "- **Architecture:** 8 layers (5 Convolution + 3 Fully Connected)\n",
        "- **Usage today:** Foundational architecture, but newer models have surpassed it\n",
        "\n",
        "**3) VGGNet:**\n",
        "- **Designed for:** Image classification\n",
        "- **Architecture:** 16 or 19 layers of small 3x3 filters\n",
        "- **Usage today:** Still used for *feature extraction* and in *transfer learning*\n",
        "\n",
        "**4) GooLeNet:**\n",
        "- **Designed for:** Image classification\n",
        "- **Architecture:** 22 layers with *Inception Modules* (multiple filter sizes at each layer)\n",
        "- **Usage today:** Still relevant in modern *Inception-based* models\n",
        "\n",
        "**5) ResNet:**\n",
        "- **Designed for:** Image classification\n",
        "- **Architecture:** 50, 101 or 152 layers with *Residual Connections* (skip connections)\n",
        "- **Usage today:** Widely used in both research and industry. Foundation for many modern models\n",
        "\n",
        "**6) Xception:**\n",
        "- **Designed for:** Image classification\n",
        "- **Architecture:** 36 layers with *Depthwise Separable Convolutions*\n",
        "- **Usage today:** Used in real-word applications like *image processing* and *object detection*\n",
        "\n",
        "**7) SENet:**\n",
        "- **Designed for:** Image classification\n",
        "- **Architecture:** Adds *Squeeze and Excitation (SE)* blocks to other architectures\n",
        "- **Usage today:** Used in many modern networks (ResNeXt, EfficientNet)"
      ],
      "metadata": {
        "id": "zt-bdqe0nyY9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing a ResNet-34 CNN using Keras\n",
        "First, create a `ResidualUnit` layer:"
      ],
      "metadata": {
        "id": "IWPyNBioqUhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, strides=1,\n",
        "                        padding=\"same\", kernel_initializer=\"he_normal\",\n",
        "                        use_bias=False)\n",
        "\n",
        "class ResidualUnit(tf.keras.layers.Layer):\n",
        "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.activation = tf.keras.activations.get(activation)\n",
        "        self.main_layers = [\n",
        "            DefaultConv2D(filters, strides=strides),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            self.activation,\n",
        "            DefaultConv2D(filters),\n",
        "            tf.keras.layers.BatchNormalization()\n",
        "        ]\n",
        "        self.skip_layers = []\n",
        "        if strides > 1:\n",
        "            self.skip_layers = [\n",
        "                DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
        "                tf.keras.layers.BatchNormalization()\n",
        "            ]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        Z = inputs\n",
        "        for layer in self.main_layers:\n",
        "            Z = layer(Z)\n",
        "        skip_Z = inputs\n",
        "        for layer in self.skip_layers:\n",
        "            skip_Z = layer(skip_Z)\n",
        "        return self.activation(Z + skip_Z)"
      ],
      "metadata": {
        "id": "7oOfoe1lluXn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now build a ResNet-34 using a `Sequential` model, since it is jsut a long sequence of layers, we can treat each residual unit as a single layer now that we have the `ResidualUnit` class:"
      ],
      "metadata": {
        "id": "vF5BWpo9qlTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    DefaultConv2D(64, kernel_size=7, strides=2, input_shape=[224, 224, 3]),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Activation(\"relu\"),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\"),\n",
        "])\n",
        "prev_filters = 64\n",
        "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
        "    strides = 1 if filters == prev_filters else 2\n",
        "    model.add(ResidualUnit(filters, strides=strides))\n",
        "    prev_filters = filters\n",
        "\n",
        "model.add(tf.keras.layers.GlobalAvgPool2D())\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))"
      ],
      "metadata": {
        "id": "x2qII-fYqyfR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Pretrained Models from Keras\n",
        "In general, we don't have to implement standard models like *GooLeNet* or *ResNet* manually since pretrained networks are available in the `tf.keras.applications` package.\n",
        "\n",
        "For example, we can load the ResNet-50 model, pretrained on ImageNet, using:"
      ],
      "metadata": {
        "id": "MmKmfx1wq4JK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.applications.ResNet50(weights=\"imagenet\")"
      ],
      "metadata": {
        "id": "kdoiTSw0qd6j"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can create a ResNet-50 model and download weights pretrained on the ImageNet dataset. To use it, first ensure that the images have the right size: ResNet-50 model expects 224x224 pixel images. So, we can use Keras's `Resizing` layer to resize 2 sample images:"
      ],
      "metadata": {
        "id": "_ET32CubrPhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images = tf.keras.backend.constant(load_sample_images()[\"images\"])\n",
        "images_resized = tf.keras.layers.Resizing(height=224, width=224,\n",
        "                                          crop_to_aspect_ratio=True)(images)"
      ],
      "metadata": {
        "id": "YrQbv6U5rMfc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pretrained models assume that the iamges are preprocessed in a specific way. In some cases they may expect the inputs to be scaled from 0 to 1, or\n",
        "from –1 to 1, and so on. Each model provides a `preprocess_input` function that can be used to preprocess the images:"
      ],
      "metadata": {
        "id": "k_UfwB-er1IS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.applications.resnet50.preprocess_input(images_resized)"
      ],
      "metadata": {
        "id": "cWPJ1n4drp-_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use the pretrained model to make predictions:"
      ],
      "metadata": {
        "id": "fd2UWpA2sGcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y_proba = model.predict(inputs)\n",
        "Y_proba.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWztGcI4sFBi",
        "outputId": "b955f519-1520-47f1-c94f-82f8bb099658"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As usual, the output `Y_proba` is a matrix with 1 row per image and 1 column per class (in this case, there are 1,000 classes). To display the top *K* predictions, use the `decode_predictions` function, which returns an array containing the class identifier, its name, and the corresponding confidence score for each iamge:"
      ],
      "metadata": {
        "id": "bEAxM--3sO9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_K = tf.keras.applications.resnet50.decode_predictions(Y_proba, top=3)\n",
        "\n",
        "for image_index in range(len(images)):\n",
        "    print(f\"Image #{image_index}\")\n",
        "    for class_id, name, y_proba in top_K[image_index]:\n",
        "        print(f\"  {class_id} - {name:12s} {y_proba:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYoNxlmzsKZp",
        "outputId": "56268eca-72b5-4023-d84c-8cb6744540c5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image #0\n",
            "  n03877845 - palace       54.69%\n",
            "  n03781244 - monastery    24.71%\n",
            "  n02825657 - bell_cote    18.55%\n",
            "Image #1\n",
            "  n04522168 - vase         32.67%\n",
            "  n11939491 - daisy        17.82%\n",
            "  n03530642 - honeycomb    12.04%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pretrained Models for Transfer Learning\n",
        "If we want to build an image classifier but do not have enough data to train it from scratch, it is often a good idea to reuse the lower layers of a pretrained model. For example, let's train a model to classify pictures of flowers, reusing a pretrained Xception model.\n",
        "\n",
        "First, load the flowers dataset using \"TensorFlow Datasets\":"
      ],
      "metadata": {
        "id": "93XXejWWsqFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "dataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)\n",
        "dataset_size = info.splits[\"train\"].num_examples # 3670\n",
        "class_names = info.features[\"label\"].names # ['dandelion', 'daisy', 'tulips', 'sunflowers', 'roses']\n",
        "n_classes = info.features[\"label\"].num_classes # 5"
      ],
      "metadata": {
        "id": "-ehVLYb5tA55"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since there is only a \"train\" dataset, with no test or validation set, we need to split the training set. We can call `tfds.load` again, but this time taking the first 10% of the dataset for testing, next 15% for validation and the remaining 75% for training:"
      ],
      "metadata": {
        "id": "HAH946eZtIam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_set_raw, valid_set_raw, train_set_raw = tfds.load(\n",
        "    \"tf_flowers\",\n",
        "    split=[\"train[:10%]\", \"train[10%:25%]\", \"train[25%:]\"],\n",
        "    as_supervised=True)"
      ],
      "metadata": {
        "id": "G5NWsxw6slWT"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All 3 datasets contain individual iamges. We need to batch them, but first we need to ensure that they all have the same size else batching will fail. We can use a `Resizing` layerfor this. We must also call the `tf.keras.applications.xception.preprocess_input` function to preprocess the images appropriately for the Xception mode. Lastly, we'll shuffle the training set and use prefetching:"
      ],
      "metadata": {
        "id": "RpklAsRCtirD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "batch_size = 32\n",
        "preprocess = tf.keras.Sequential([\n",
        "    tf.keras.layers.Resizing(height=224, width=224, crop_to_aspect_ratio=True),\n",
        "    tf.keras.layers.Lambda(tf.keras.applications.xception.preprocess_input)\n",
        "])\n",
        "train_set = train_set_raw.map(lambda X, y: (preprocess(X), y))\n",
        "train_set = train_set.shuffle(1000, seed=42).batch(batch_size).prefetch(1)\n",
        "valid_set = valid_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)\n",
        "test_set = test_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)"
      ],
      "metadata": {
        "id": "Su0X4_DdtYCt"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now each batch contains 32 images, all of them being 224x224 pixels, with pixel values ranging from -1 to 1.\n",
        "\n",
        "Since the dataset is not very huge, a bit of data augmentation will help. Let's create a data augmentation model that we will embed in the  final model. During training, it will randomly flip the images horizontally, rotate them a little bit, and tweak the constrast:"
      ],
      "metadata": {
        "id": "vEm2nMWpt52N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=42),\n",
        "    tf.keras.layers.RandomRotation(factor=0.05, seed=42),\n",
        "    tf.keras.layers.RandomContrast(factor=0.2, seed=42)\n",
        "])"
      ],
      "metadata": {
        "id": "7S8Eb_xqt3PJ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next let’s load an Xception model, pretrained on ImageNet. We exclude the\n",
        "top of the network by setting `include_top=False`. This excludes the global\n",
        "average pooling layer and the dense output layer. We then add our own\n",
        "global average pooling layer (feeding it the output of the base model),\n",
        "followed by a dense output layer with one unit per class, using the softmax\n",
        "activation function:"
      ],
      "metadata": {
        "id": "qL11hBg_ucqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "base_model = tf.keras.applications.xception.Xception(weights=\"imagenet\",\n",
        "                                                     include_top=False)\n",
        "avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
        "output = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
        "model = tf.keras.Model(inputs=base_model.input, outputs=output)"
      ],
      "metadata": {
        "id": "XpZOR9SeuUs2"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is usually a good idea to freeze the weights of the pretrained layers, at least at the beginning of training:"
      ],
      "metadata": {
        "id": "dsWKXnzrusNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in base_model.layers:\n",
        "  layer.trainable = False"
      ],
      "metadata": {
        "id": "wDxvY0vsumy4"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can compile the model and start training:"
      ],
      "metadata": {
        "id": "N1TfJnGfu17a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, validation_data=valid_set, epochs=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGiAqfBPuy7t",
        "outputId": "0d498877-af79-43a8-ce1b-b05ee5ab3302"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m796s\u001b[0m 9s/step - accuracy: 0.7201 - loss: 0.8890 - val_accuracy: 0.8566 - val_loss: 0.5970\n",
            "Epoch 2/3\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m817s\u001b[0m 9s/step - accuracy: 0.9026 - loss: 0.3612 - val_accuracy: 0.8367 - val_loss: 0.8240\n",
            "Epoch 3/3\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m859s\u001b[0m 9s/step - accuracy: 0.9306 - loss: 0.2099 - val_accuracy: 0.8276 - val_loss: 0.7655\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After a few epochs, the model's validation accuracy will stop improving. This means that the top layers are now pretty well trained, and we can now unfreeze some of the base model's top layers then continue trianing. For example, let's unfreeze layers 56 and above:"
      ],
      "metadata": {
        "id": "tudI5IcPvJe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in base_model.layers[56:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, validation_data=valid_set, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFCZGXxJu5ye",
        "outputId": "20b8d6d6-dc0d-4981-caef-1bf09c99b8e4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1607s\u001b[0m 18s/step - accuracy: 0.9334 - loss: 0.2102 - val_accuracy: 0.8875 - val_loss: 0.5031\n",
            "Epoch 2/3\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1592s\u001b[0m 19s/step - accuracy: 0.9816 - loss: 0.0597 - val_accuracy: 0.9002 - val_loss: 0.3643\n",
            "Epoch 3/3\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1623s\u001b[0m 18s/step - accuracy: 0.9968 - loss: 0.0147 - val_accuracy: 0.8947 - val_loss: 0.3570\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification and Localization\n",
        "Localizing an object in a picture can be expressed as a regression task: to predict a bounding box around the object. A common approach is to predict the horizontal and vertical coordinates of the object's center, as well as its height and width. Meaning that we have 4 numbers to predict. It does not require much change to the model; we just need to add a second dense output layer with 4 units, and it can be trained using MSE loss:"
      ],
      "metadata": {
        "id": "xmIchkVcve-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "base_model = tf.keras.applications.xception.Xception(weights=\"imagenet\",\n",
        "                                                     include_top=False)\n",
        "avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
        "class_output = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
        "loc_output = tf.keras.layers.Dense(4)(avg)\n",
        "model = tf.keras.Model(inputs=base_model.input,\n",
        "                       outputs=[class_output, loc_output])\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)  # added this line\n",
        "model.compile(loss=[\"sparse_categorical_crossentropy\", \"mse\"],\n",
        "              loss_weights=[0.8, 0.2],  # depends on what you care most about\n",
        "              optimizer=optimizer, metrics=[\"accuracy\", \"mse\"])"
      ],
      "metadata": {
        "id": "GMKerLSpvbjz"
      },
      "execution_count": 31,
      "outputs": []
    }
  ]
}