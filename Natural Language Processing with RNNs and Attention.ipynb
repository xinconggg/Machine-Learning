{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinconggg/Machine-Learning/blob/main/Natural%20Language%20Processing%20with%20RNNs%20and%20Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "Have to use Keras 2 instead of 3. To do that, set the `TF_USE_LEGACY_KERAS` environment variable to \"1\" and import the `tf_keras package`. This ensures that `tf.keras` points to `tf_keras`, which is Keras 2.*."
      ],
      "metadata": {
        "id": "0uWz9r4JSi6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
        "\n",
        "import tf_keras"
      ],
      "metadata": {
        "id": "TERT8DOaS3fT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And TensorFlow ≥ 2.8:"
      ],
      "metadata": {
        "id": "uyS2iu45S6xH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from packaging import version\n",
        "import tensorflow as tf\n",
        "\n",
        "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")"
      ],
      "metadata": {
        "id": "d424Ge8JS8hS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGX08aDcYbRl"
      },
      "source": [
        "## Generating Shakespearean Text using a Character RNN\n",
        "### Creating the Training Dataset\n",
        "Using Keras's `tf.keras.utils.get_file` function, download all of Shakespeare's works:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "assert sys.version_info >= (3, 7)"
      ],
      "metadata": {
        "id": "Aq-iW3MFEsKO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
        "\n",
        "import tf_keras"
      ],
      "metadata": {
        "id": "9mlPbItdEvRH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from packaging import version\n",
        "import tensorflow as tf\n",
        "\n",
        "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")"
      ],
      "metadata": {
        "id": "vo0gx1SsExPD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PJnbyjRdYKsn"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "shakespeare_url = \"https://homl.info/shakespeare\"  # shortcut URL\n",
        "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
        "with open(filepath) as f:\n",
        "    shakespeare_text = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PapZXAQLYxok"
      },
      "source": [
        "Print the first few lines to ensure that the code is working:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aifItu8vYu00",
        "outputId": "6ebb9fa3-0ee6-4f5e-9f2b-87650d413928"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n"
          ]
        }
      ],
      "source": [
        "print(shakespeare_text[:80])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PffK0ii1Y6sW"
      },
      "source": [
        "Now use a `tf.keras.layers.TextVectorization` layer to encode this text. Set `split=\"character\"` to convert the text to lowercase:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "elHUG4J5Y0FH"
      },
      "outputs": [],
      "source": [
        "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\",\n",
        "                                                   standardize=\"lower\")\n",
        "text_vec_layer.adapt([shakespeare_text])\n",
        "encoded = text_vec_layer([shakespeare_text])[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6e4LYD_ZT4-"
      },
      "source": [
        "Each character is now mapped to an integer, starting at 2, since the `TextVectorization` layer reserved the value 0 for padding tokens and reversed 1 for unknown characters. Since we won't need either of these tokens, we can substract 2 from the character IDs and compute the number of distinct characters and the total number of characters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VxUcX8hoZMnR"
      },
      "outputs": [],
      "source": [
        "encoded -= 2  # drop tokens 0 (pad) and 1 (unknown), which we will not use\n",
        "n_tokens = text_vec_layer.vocabulary_size() - 2  # number of distinct chars = 39\n",
        "dataset_size = len(encoded)  # total number of chars = 1,115,394"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYRaAIC5asDp"
      },
      "source": [
        "We can turn this very long sequence into a dataset of windows that we can then use to train a sequence-to-sequence RNN. Write a small function to convert a long sequence of character IDs into a dataset of input/target window pairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2up_lqm3aYQP"
      },
      "outputs": [],
      "source": [
        "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
        "    ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(100_000, seed=seed)\n",
        "    ds = ds.batch(batch_size)\n",
        "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZbPLyi5bBxA"
      },
      "source": [
        "Walk through of function:\n",
        "- It takes a sequence as input (i.e., the encoded text), and creates a dataset containing all the windows of the desired length.\n",
        "- It increases the length by one since we need the next character for the target.\n",
        "- It then shuffles the windows, batches them, splits them into input/output pairs, and activates prefetching."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfaumRMTbZLJ"
      },
      "source": [
        "Create the training(90%), validation(5%) and test(5%) set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7JG99ZZma8gX"
      },
      "outputs": [],
      "source": [
        "length = 100\n",
        "tf.random.set_seed(42)\n",
        "train_set = to_dataset(encoded[:1_000_000], length=length, shuffle=True,\n",
        "                       seed=42)\n",
        "valid_set = to_dataset(encoded[1_000_000:1_060_000], length=length)\n",
        "test_set = to_dataset(encoded[1_060_000:], length=length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0k-Zd5wbmdv"
      },
      "source": [
        "### Building and Training the Char-RNN Model\n",
        "Since the dataset is large, and the modeling language is a difficult task, we need more than a simple RNN with a few recurrent neurons. Build and train a model with one GRU layer composed of 128 units:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_4wQQ7YbdnS",
        "outputId": "d7020d33-a154-4e90-e645-56c4dafd3fdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "31247/31247 [==============================] - 440s 13ms/step - loss: 1.4006 - accuracy: 0.5712 - val_loss: 1.5975 - val_accuracy: 0.5349\n",
            "Epoch 2/3\n",
            "31247/31247 [==============================] - 416s 13ms/step - loss: 1.2978 - accuracy: 0.5957 - val_loss: 1.5738 - val_accuracy: 0.5425\n",
            "Epoch 3/3\n",
            "31247/31247 [==============================] - 414s 13ms/step - loss: 1.2785 - accuracy: 0.6003 - val_loss: 1.5671 - val_accuracy: 0.5432\n"
          ]
        }
      ],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
        "    tf.keras.layers.GRU(128, return_sequences=True),\n",
        "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
        "    \"my_shakespeare_model.keras\", monitor=\"val_accuracy\", save_best_only=True)\n",
        "history = model.fit(train_set, validation_data=valid_set, epochs= 3,\n",
        "                    callbacks=[model_ckpt])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zseyxEsMeEMv"
      },
      "source": [
        "Walk through of code:\n",
        "- Use `Embedding` layer as the first layer to encode the character IDs. The `Embedding` layer's number of input dimensions is the number of distinct character IDs and the number of output dimensions is a hyperparameter that can be tuned. The inputs of the `Embedding` layer will be 2D tensors of shape and the output will be 3D tensor of shape.\n",
        "- The `Dense` layer is used for the output layer: it must have 39 units (`n_tokens`) because there are 39 distinct characters in the text. The 39 output probabilities should sum up to 1 at each time step, so we apply the **softmax** activation function to the outputs of the `Dense` layer.\n",
        "- Lastly, we compile the model using the `sparse_categorical_crossentropy` loss and a **Nadam** optimizer and train the model for several epochs using a `ModelCheckpoint` callback to save the best model as training progresses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnCIW5KVfXGX"
      },
      "source": [
        "Since the model does not handle text preprocessing, let's wrap it in a final model containing the `tf.keras.layers.TextVectorization` layer as the first layer, plus a `tf.keras.layers.Lambda` layer to subtract 2 from the character IDs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "P9pW9enjd2Ce"
      },
      "outputs": [],
      "source": [
        "shakespeare_model = tf.keras.Sequential([\n",
        "    text_vec_layer,\n",
        "    tf.keras.layers.Lambda(lambda X: X - 2),\n",
        "    model\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSCiPD37frJ4"
      },
      "source": [
        "Use it to predict the next character in a sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jQ3Fr_j_fqBQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5247d93-5285-4b3d-cf30-ffc81c8851ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 375ms/step\n",
            "Predicted character: e\n"
          ]
        }
      ],
      "source": [
        "# Tokenize at the character level (if required by your model)\n",
        "input_text = tf.convert_to_tensor([\"To be or not to b\"])\n",
        "\n",
        "# Predict probabilities\n",
        "y_proba = shakespeare_model.predict(input_text)[0, -1]\n",
        "\n",
        "# Find the most probable character (and check if it corresponds to the expected character 'e')\n",
        "y_pred = tf.argmax(y_proba)  # choose the most probable character ID\n",
        "\n",
        "# Retrieve the corresponding character\n",
        "character = text_vec_layer.get_vocabulary()[y_pred + 2]  # Adjust indexing for padding\n",
        "\n",
        "print(f\"Predicted character: {character}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdrvAJUIqA8r"
      },
      "source": [
        "### Generating Fake Shakespearean Text\n",
        "To generate new text using the Char-RNN model, we could feed it some text, make the model predict\n",
        "the most likely next letter, add it to the end of the text, then give the extended text to the model to\n",
        "guess the next letter, and so on. This is called *greedy decoding*. But in practice this often leads to the same words being repeated over and over again. Instead, we can sample the next character\n",
        "randomly, with a probability equal to the estimated probability, using TensorFlow’s\n",
        "`tf.random.categorical()` function. This will generate more diverse and interesting text. The\n",
        "`categorical()` function samples random class indices, given the class log probabilities (logits). For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Dpbnf8Xrfv5p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5396a505-86ea-4acd-ca43-4b289756259f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 8), dtype=int64, numpy=array([[0, 0, 1, 1, 1, 0, 0, 0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "log_probas = tf.math.log([[0.5, 0.4, 0.1]])  # probas = 50%, 40%, and 10%\n",
        "tf.random.set_seed(42)\n",
        "tf.random.categorical(log_probas, num_samples=8)  # draw 8 samples"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To have more control over the diversity of the generated text, we can divide the logits by a number called the *temperature*, which we can tweak as we wish. A temperature close to 0 favors high-probability characters, while a high temperature gives all characters an equal probability. Lower temperature are typically preferred when generating a fairly rigid and precise text, such as mathematical equations, while higher temperatures are preferred when generating more diverse and creative text. The following `next_char()` custom helper function uses this approach to pick the next character to add to the input text:"
      ],
      "metadata": {
        "id": "3gAaqOJRiAeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def next_char(text, temperature=1):\n",
        "    y_proba = shakespeare_model.predict([text])[0, -1:]\n",
        "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
        "    return text_vec_layer.get_vocabulary()[char_id + 2]"
      ],
      "metadata": {
        "id": "09wwkBr2ix4S"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can write another helper function that will repeatedly call the `next_char()` function to get the next character, and append it to the given text:"
      ],
      "metadata": {
        "id": "TpM8_gKwi4CR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extend_text(text, n_chars=50, temperature=1):\n",
        "    for _ in range(n_chars):\n",
        "        text += next_char(text, temperature)\n",
        "    return text"
      ],
      "metadata": {
        "id": "D5q31AYyi0F6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can try generating some text with different temperature values:"
      ],
      "metadata": {
        "id": "v_PzgDRUjEiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Temperature = 0.01\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "input_text = tf.convert_to_tensor([\"To be or not to be\"])\n",
        "print(extend_text(input_text, temperature=0.01))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rRf1IQjjBax",
        "outputId": "235f5b85-71fa-44ff-d137-2161639d59a6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 364ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "tf.Tensor([b'To be or not to be a shall the duke\\nwill not show me to the duke of '], shape=(1,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Temperature = 1\n",
        "input_text = tf.convert_to_tensor([\"To be or not to be\"])\n",
        "print(extend_text(input_text, temperature=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGv2SRZLjQbi",
        "outputId": "477de318-a186-4940-8c7d-221536427be0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "tf.Tensor([b'To be or not to begun it off\\ntake the battle sprittinous sonds\\nas te'], shape=(1,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Temperature = 100\n",
        "input_text = tf.convert_to_tensor([\"To be or not to be\"])\n",
        "print(extend_text(input_text, temperature=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gDMgMe3jUhJ",
        "outputId": "a51927d4-3228-4cd2-c8e7-4e3f9960bf00"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "tf.Tensor([b\"To be or not to bepevicm-v lv!?$ez?gmjz :3?ljb'va;!td&\\ni.ur3l'-j!3eu\"], shape=(1,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stateful RNN\n",
        "A stateful RNN only makes sense if each input sequence in a batch starts exactly where the correspodning sequence in the previous batch left off. So, we need to use sequential and nonoverlapping input sequences rather than the shuffled and overlapping sequences used to train stateless RNNs. Hence, when creating the `tf.data.Dataset`, we must use `shift=length` instead of `shift=1` when calling the `window()` method. Take note that we must not call the `shuffle()` method.\n",
        "\n",
        "The following `to_dateset_for_stateful_rnn()` utility function uses the strategy (`batch(1)`), to prepare a dataset for a stateful RNN:"
      ],
      "metadata": {
        "id": "2HVr9TfOjZKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_dataset_for_stateful_rnn(sequence, length, batch_size=64):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
        "    ds = ds.window(length + 1, shift=length, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda window: window.batch(length + 1))\n",
        "    # Set a fixed batch size here\n",
        "    ds = ds.batch(batch_size, drop_remainder=True)\n",
        "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)\n",
        "\n",
        "stateful_train_set = to_dataset_for_stateful_rnn(encoded[:1_000_000], length, batch_size=64)\n",
        "stateful_valid_set = to_dataset_for_stateful_rnn(encoded[1_000_000:1_060_000], length, batch_size=64)\n",
        "stateful_test_set = to_dataset_for_stateful_rnn(encoded[1_060_000:], length, batch_size=64)"
      ],
      "metadata": {
        "id": "WkO-EOCqjWu0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create the stateful RNN, we need to set `stateful=True` when creating each recurrent layer. Since the stateful RNN needs to know the batch size, we must set the `batch_input_shape` argument in the first layer. Note that the second dimension can be left unspecified since the input sequences can have any length\""
      ],
      "metadata": {
        "id": "0usIJpS7k9pX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    Input(shape=(None,), batch_size=64),  # Specify the batch size and sequence length\n",
        "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
        "    tf.keras.layers.GRU(128, return_sequences=True, stateful=True),  # No need for batch_input_shape here\n",
        "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
        "])"
      ],
      "metadata": {
        "id": "MRI37EIyk4By"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the end of each epoch, we need to reset the states before we go back to the beginning of the text. For this, we can use a small custom Keras callback:"
      ],
      "metadata": {
        "id": "V5OpCsbTmCC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResetStatesCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        # Iterate over each RNN layer (GRU in this case) and reset states\n",
        "        for layer in self.model.layers:\n",
        "            if isinstance(layer, tf.keras.layers.RNN):\n",
        "                layer.reset_states()"
      ],
      "metadata": {
        "id": "1uCCB_Jhvnd7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can compile the model and train it using the callback function:"
      ],
      "metadata": {
        "id": "eA_i76X7mf9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(stateful_train_set, validation_data=stateful_valid_set,\n",
        "                    epochs=10, callbacks=[ResetStatesCallback(), model_ckpt])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClBEYh0wmP0Z",
        "outputId": "3e5edb63-3278-4b45-9f4d-9e8638dcceec"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "156/156 [==============================] - 7s 30ms/step - loss: 3.0538 - accuracy: 0.1666 - val_loss: 2.7454 - val_accuracy: 0.2431\n",
            "Epoch 2/10\n",
            "156/156 [==============================] - 5s 34ms/step - loss: 2.5050 - accuracy: 0.2890 - val_loss: 2.3904 - val_accuracy: 0.2965\n",
            "Epoch 3/10\n",
            "156/156 [==============================] - 4s 27ms/step - loss: 2.3206 - accuracy: 0.3204 - val_loss: 2.2514 - val_accuracy: 0.3359\n",
            "Epoch 4/10\n",
            "156/156 [==============================] - 4s 25ms/step - loss: 2.2004 - accuracy: 0.3563 - val_loss: 2.1436 - val_accuracy: 0.3654\n",
            "Epoch 5/10\n",
            "156/156 [==============================] - 5s 33ms/step - loss: 2.1030 - accuracy: 0.3802 - val_loss: 2.0670 - val_accuracy: 0.3811\n",
            "Epoch 6/10\n",
            "156/156 [==============================] - 4s 26ms/step - loss: 2.0211 - accuracy: 0.4016 - val_loss: 2.0090 - val_accuracy: 0.4017\n",
            "Epoch 7/10\n",
            "156/156 [==============================] - 4s 27ms/step - loss: 1.9531 - accuracy: 0.4210 - val_loss: 1.9646 - val_accuracy: 0.4149\n",
            "Epoch 8/10\n",
            "156/156 [==============================] - 4s 29ms/step - loss: 1.8965 - accuracy: 0.4370 - val_loss: 1.9288 - val_accuracy: 0.4237\n",
            "Epoch 9/10\n",
            "156/156 [==============================] - 4s 26ms/step - loss: 1.8486 - accuracy: 0.4506 - val_loss: 1.8982 - val_accuracy: 0.4342\n",
            "Epoch 10/10\n",
            "156/156 [==============================] - 5s 33ms/step - loss: 1.8079 - accuracy: 0.4618 - val_loss: 1.8720 - val_accuracy: 0.4418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting the Stateful RNN to a Stateless RNN then using it\n",
        "To use the model with different batch sizes, we need to create a stateless copy:"
      ],
      "metadata": {
        "id": "c4rK55elmxDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stateless_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
        "    tf.keras.layers.GRU(128, return_sequences=True),\n",
        "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
        "])"
      ],
      "metadata": {
        "id": "V94LT3vqmnGx"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To set the weights, we need to firstly build the model so that the weights get created:"
      ],
      "metadata": {
        "id": "xCEA9u6pm_Bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stateless_model.build(tf.TensorShape([None, None]))"
      ],
      "metadata": {
        "id": "6_qcHILRm-Ox"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stateless_model.set_weights(model.get_weights())"
      ],
      "metadata": {
        "id": "zIsr5gRe1zSH"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_model = tf.keras.Sequential([\n",
        "    text_vec_layer,\n",
        "    tf.keras.layers.Lambda(lambda X: X - 2),  # no <PAD> or <UNK> tokens\n",
        "    stateless_model\n",
        "])"
      ],
      "metadata": {
        "id": "7sl7o6yo11nE"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "text = tf.convert_to_tensor([\"To be or not to be\"])\n",
        "print(extend_text(text, temperature=0.01))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VxTKsU113KE",
        "outputId": "284e8103-657a-4f97-d428-9fef0d4dfeea"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 382ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "tf.Tensor([b'To be or not to be her for the senventio:\\ni will her for the senvent'], shape=(1,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis\n",
        "**Sentiment Analysis** is a Natural Language Processing (NLP) technique used to determine the emotional tone or opinion expressed in a piece of text. It classifies text as positive, negative, or neutral, helping to understand the author's sentiment or attitude.\n",
        "\n",
        "This technique is widely applied in areas like:\n",
        "- **Social media monitoring** to analyze customer feedback.\n",
        "- **Product reviews** to assess user satisfaction.\n",
        "- **Customer service chats** to gauge emotions and improve responses.\n",
        "\n",
        "By leveraging machine learning models and NLP, sentiment analysis can extract insights from large volumes of text data, improving decision-making in marketing, customer service, and brand management."
      ],
      "metadata": {
        "id": "I4q8qREunSvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the IMDb dataset using the TensorFlow Datasets library and use the first 90% of the training set for training and the remaining 10% for validation:"
      ],
      "metadata": {
        "id": "WwYuU3P3oYa8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "raw_train_set, raw_valid_set, raw_test_set = tfds.load(\n",
        "    name=\"imdb_reviews\",\n",
        "    split=[\"train[:90%]\", \"train[90%:]\", \"test\"],\n",
        "    as_supervised=True\n",
        ")\n",
        "tf.random.set_seed(42)\n",
        "train_set = raw_train_set.shuffle(5000, seed=42).batch(32).prefetch(1)\n",
        "valid_set = raw_valid_set.batch(32).prefetch(1)\n",
        "test_set = raw_test_set.batch(32).prefetch(1)"
      ],
      "metadata": {
        "id": "4jw2LiLAnLoJ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect a few reviews:"
      ],
      "metadata": {
        "id": "E9JLjfqGol36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for review, label in raw_train_set.take(4):\n",
        "    print(review.numpy().decode(\"utf-8\")[:200], \"...\")\n",
        "    print(\"Label:\", label.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99h0riuFoj9k",
        "outputId": "43ab6edf-a8de-4275-8489-6ca510c0ea6d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting  ...\n",
            "Label: 0\n",
            "I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However  ...\n",
            "Label: 0\n",
            "Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Moun ...\n",
            "Label: 0\n",
            "This is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big arm-chair and mellow for a couple of hours. Wonderful perf ...\n",
            "Label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some reviews are easy to classify: the first review includes the word \"terrible movie\" in the first sentence. However, in many cases it is not easy to classify reviews: the third review start of positively but it ultimately got a negative review (label 0).\n",
        "\n",
        "To build a model for this task, we need to preprocess the text and chop it into words instead of characters using `tf.keras.layers.TextVectorization`. Limit the vocabulary to 1,000 tokens, including the most frequent 998 words plus a padding token and a token for unknown words:"
      ],
      "metadata": {
        "id": "Ccwwt5VsovIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 1000\n",
        "text_vec_layer = tf.keras.layers.TextVectorization(max_tokens=vocab_size)\n",
        "text_vec_layer.adapt(train_set.map(lambda reviews, labels: reviews))"
      ],
      "metadata": {
        "id": "FLtRRnkJoqFB"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the model and train it:"
      ],
      "metadata": {
        "id": "GifMsbZOpx8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 128\n",
        "tf.random.set_seed(42)\n",
        "model = tf.keras.Sequential([\n",
        "    text_vec_layer,\n",
        "    tf.keras.layers.Embedding(vocab_size, embed_size),\n",
        "    tf.keras.layers.GRU(128),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, validation_data=valid_set, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7y2chD3pnIC",
        "outputId": "3f943241-5551-499c-c353-6a1400629a66"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "704/704 [==============================] - 39s 50ms/step - loss: 0.6935 - accuracy: 0.5012 - val_loss: 0.6931 - val_accuracy: 0.5016\n",
            "Epoch 2/5\n",
            "704/704 [==============================] - 25s 35ms/step - loss: 0.6928 - accuracy: 0.5044 - val_loss: 0.6944 - val_accuracy: 0.4992\n",
            "Epoch 3/5\n",
            "704/704 [==============================] - 23s 33ms/step - loss: 0.6917 - accuracy: 0.5027 - val_loss: 0.6951 - val_accuracy: 0.5004\n",
            "Epoch 4/5\n",
            "704/704 [==============================] - 24s 33ms/step - loss: 0.6878 - accuracy: 0.5196 - val_loss: 0.6040 - val_accuracy: 0.6956\n",
            "Epoch 5/5\n",
            "704/704 [==============================] - 23s 33ms/step - loss: 0.4135 - accuracy: 0.8117 - val_loss: 0.3332 - val_accuracy: 0.8608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Masking\n",
        "In Keras, making a model ignore padding tokens is simple: set `mask_zero=True` in the `Embedding` layer. This creates a mask tensor that marks padding tokens (ID 0) as False and other tokens as True. The mask automatically propagates to subsequent layers that support masking (i.e., layers with `supports_masking=True`).\n",
        "\n",
        "For example:\n",
        "\n",
        "- **Recurrent layers** use the mask to ignore padding steps by copying the output from the previous time step.\n",
        "- Mask propagation continues through layers with return_sequences=True, but stops at the first layer with `return_sequences=False`.\n",
        "\n",
        "In a sentiment analysis model with a GRU layer, the mask will be used by the GRU to handle padding but won’t propagate beyond it if `return_sequences=False`.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uJtp3HS-qe8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using masking layers and automatic mask propagation is effective for simple models, but it may not work for more complex models, such as those that mix `Conv1D` layers with recurrent layers. In these cases, you need to manually compute the mask and pass it to the appropriate layers, either using the functional API or the subclassing API. The model described is equivalent to the previous one, but it is built using the functional API. It explicitly handles masking and adds dropout to address slight overfitting from the previous model:"
      ],
      "metadata": {
        "id": "2k206i-Iysl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 128\n",
        "tf.random.set_seed(42)\n",
        "model = tf.keras.Sequential([\n",
        "    text_vec_layer,\n",
        "    tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True),\n",
        "    tf.keras.layers.GRU(128),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, validation_data=valid_set, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cA02wb1dy0Q4",
        "outputId": "d4380950-2256-425d-fe9f-4d6278520534"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "704/704 [==============================] - 43s 55ms/step - loss: 0.5346 - accuracy: 0.7258 - val_loss: 0.4913 - val_accuracy: 0.7744\n",
            "Epoch 2/5\n",
            "704/704 [==============================] - 26s 37ms/step - loss: 0.3538 - accuracy: 0.8518 - val_loss: 0.3237 - val_accuracy: 0.8688\n",
            "Epoch 3/5\n",
            "704/704 [==============================] - 25s 36ms/step - loss: 0.2959 - accuracy: 0.8766 - val_loss: 0.3233 - val_accuracy: 0.8660\n",
            "Epoch 4/5\n",
            "704/704 [==============================] - 25s 35ms/step - loss: 0.2698 - accuracy: 0.8911 - val_loss: 0.3015 - val_accuracy: 0.8708\n",
            "Epoch 5/5\n",
            "704/704 [==============================] - 25s 36ms/step - loss: 0.2545 - accuracy: 0.8991 - val_loss: 0.3070 - val_accuracy: 0.8700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another approach to masking is to feed the model with ragged tensors. Just set `ragged=True` when creating the `TextVectorization` layer:"
      ],
      "metadata": {
        "id": "EnYXgb0fzhec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer_ragged = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=vocab_size, ragged=True)\n",
        "text_vec_layer_ragged.adapt(train_set.map(lambda reviews, labels: reviews))\n",
        "text_vec_layer_ragged([\"Great movie!\", \"This is DiCaprio's best role.\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MDh1F1Mzr_B",
        "outputId": "fdeca9a7-facc-4445-b811-de2c4af06dce"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[86, 18], [11, 7, 1, 116, 217]]>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## An Encoder-Decoder Network for Neural Machine Translation\n",
        "Build a simple Neural Machine Translator (NMT) model as follows: English sentences are fed as inputs to the encoder and the decoder outputs the Spanish translation. Note that the Spanish translations are also used as inputs to the decoder during training but shifted back by one step. In other words, during training the decoder is given as input the word that it should have output at the previous step. This is called *Teacher Forcing*, which is a technique that significantly speeds up trianing and improves the model's performance.\n",
        "\n",
        "Each word is initially represented by its ID (e.g., 954 for the word \"soccer\"). Next, an `Embedding` layer returns the word embedding. These word embeddings are then fed into the encoder and the decoder.\n",
        "\n",
        "At each step, the decoder outputs a score for each word in the output vocabulary (i.e., Spanish), then the softmax activation function turns these scores into probabilities. For example, at the first step the\n",
        "word “Me” may have a probability of 7%, “Yo” may have a probability of 1%, and so on. The word\n",
        "with the highest probability is output."
      ],
      "metadata": {
        "id": "s7a0SSBY0SS8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To build the model, we first need to download a dataset of English/Spanish sentence pairs:"
      ],
      "metadata": {
        "id": "fvuNkRglK16w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "IMAGES_PATH = Path() / \"images\" / \"nlp\"\n",
        "IMAGES_PATH.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "i94o5RJPMLGj"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
        "path = tf.keras.utils.get_file(\"spa-eng.zip\", origin=url, cache_dir=\"datasets\",\n",
        "                               extract=True)\n",
        "text = (Path(path).with_name(\"spa-eng\") / \"spa.txt\").read_text()"
      ],
      "metadata": {
        "id": "euvDEzau0P1x"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each line contains an English sentence and the corresponding Spanish translation, separated by a tab. First start by removing the Spanish characters  “¡” and “¿”, which the `TextVectorization` layer doesn't handle, then parse the sentence pairs and shuffle them. Finally, we will split them into 2 separate lists, 1 per language:"
      ],
      "metadata": {
        "id": "b4RaUkIQMYwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "text = text.replace(\"¡\", \"\").replace(\"¿\", \"\")\n",
        "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(pairs)\n",
        "sentences_en, sentences_es = zip(*pairs)  # separates the pairs into 2 lists"
      ],
      "metadata": {
        "id": "_jFeh_OxLA0v"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a look at the first 3 sentence pairs:"
      ],
      "metadata": {
        "id": "40_PWqHWM3Xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "  print(sentences_en[i], \"=>\", sentences_es[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9Sjr161M0Zv",
        "outputId": "bab156f1-9ba0-4f0b-fe49-e1cdfbb2a128"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How boring! => Qué aburrimiento!\n",
            "I love sports. => Adoro el deporte.\n",
            "Would you like to swap jobs? => Te gustaría que intercambiemos los trabajos?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, create 2 `TextVectorization` layers, 1 per language and adapt them to the text:"
      ],
      "metadata": {
        "id": "WC68lgKNNTM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 1000\n",
        "max_length = 50\n",
        "text_vec_layer_en = tf.keras.layers.TextVectorization(\n",
        "    vocab_size, output_sequence_length=max_length)\n",
        "text_vec_layer_es = tf.keras.layers.TextVectorization(\n",
        "    vocab_size, output_sequence_length=max_length)\n",
        "text_vec_layer_en.adapt(sentences_en)\n",
        "text_vec_layer_es.adapt([f\"startofseq {s} endofseq\" for s in sentences_es])"
      ],
      "metadata": {
        "id": "4nSy4h8sNBfj"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take note that:\n",
        "- Vocabulary size was limited to only 1000 since the training set is not very large and using a small value will speed up training.\n",
        "- Since all sentences in the dataset have a maximum of 50 words, `output_sequence_length` was set to 50 so that the input sequences will automatically be padded with zeros until they are all 50 tokens long. If there were any sentences with more than 50 words, they will be cropped to only 50 tokens.\n",
        "- For the Spanish text, \"startofseq\"(S0S) and \"endofseq\"(EOS) was added to each sentence when adapting the `TextVectorization` layer."
      ],
      "metadata": {
        "id": "PZH1FzQGNdja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect the first 10 tokens in both vocabularies:"
      ],
      "metadata": {
        "id": "i76KHqlKOwuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer_en.get_vocabulary()[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvUi3W2CNa2O",
        "outputId": "b54aab62-6cad-4f7b-d67c-704fb5c3880a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'the', 'i', 'to', 'you', 'tom', 'a', 'is', 'he']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer_es.get_vocabulary()[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uzAkUYsO6lC",
        "outputId": "5e8e867d-4223-4d7f-cf32-dc0b5e559387"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'startofseq', 'endofseq', 'de', 'que', 'a', 'no', 'tom', 'la']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "They both start with the padding token, unknown token, the SOS token and EOS token (only for Spanish), then the actual words, sorted by decreasing frequency."
      ],
      "metadata": {
        "id": "5DT-ymNcO8rB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the training and validation set:"
      ],
      "metadata": {
        "id": "ipQc_bS_PUuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = tf.constant(sentences_en[:100_000])\n",
        "X_valid = tf.constant(sentences_en[100_000:])\n",
        "X_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:100_000]])\n",
        "X_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[100_000:]])\n",
        "Y_train = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[:100_000]])\n",
        "Y_valid = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[100_000:]])"
      ],
      "metadata": {
        "id": "dP0H9jPXO7tx"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can build the translation model. We will use the functional API for that since the model is not sequential. It requires 2 inputs: 1 for encoder and another for decoder:"
      ],
      "metadata": {
        "id": "vzptZeiOPY6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
        "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)"
      ],
      "metadata": {
        "id": "BsYNBbodPXUU"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need to encode these sentences using the `TextVectorization` layers prepared earlier, followed by an `Embedding` layer for each language, with `mask_zero=True` to ensure masking is handled automatically:"
      ],
      "metadata": {
        "id": "wW_9ao39Pqmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 128\n",
        "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
        "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
        "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
        "                                                    mask_zero=True)\n",
        "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
        "                                                    mask_zero=True)\n",
        "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)"
      ],
      "metadata": {
        "id": "DQQvUBwwPkYB"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can create the encoder and pass the embedded inputs:"
      ],
      "metadata": {
        "id": "QuLM0bWWP5My"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = tf.keras.layers.LSTM(512, return_state=True)\n",
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings)"
      ],
      "metadata": {
        "id": "QcmsnlQdP3DN"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To keep things simple, we just used a single `LSTM` layer, but several of them can be stacked. `return_state=True` to get a reference to the layer's final state. Since the `LSTM` layer is used, the layer returns 2 states: the short-term and the long-term state, separately. Now we can use this (double) state as the initial state of the decoder:"
      ],
      "metadata": {
        "id": "NNNSW3hiP_oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)"
      ],
      "metadata": {
        "id": "-iLGQYoaP-I0"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we pass the decoder's outputs through a `Dense` layer with the softmax activation function to get the word probabilities for each step:"
      ],
      "metadata": {
        "id": "gGXJikBPQj97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
        "Y_proba = output_layer(decoder_outputs)"
      ],
      "metadata": {
        "id": "249XVXEIQjJF"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we just need to create the Keras model, compile it, then train it:"
      ],
      "metadata": {
        "id": "K4lUQDYuQudw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                       outputs=[Y_proba])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=3,\n",
        "          validation_data=((X_valid, X_valid_dec), Y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTUX76u_Qslr",
        "outputId": "d78c8954-62bc-4e87-9109-354a807ada82"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "3125/3125 [==============================] - 89s 25ms/step - loss: 2.8921 - accuracy: 0.4308 - val_loss: 2.1198 - val_accuracy: 0.5322\n",
            "Epoch 2/3\n",
            "3125/3125 [==============================] - 65s 21ms/step - loss: 1.7984 - accuracy: 0.5842 - val_loss: 1.6100 - val_accuracy: 0.6199\n",
            "Epoch 3/3\n",
            "3125/3125 [==============================] - 64s 20ms/step - loss: 1.3965 - accuracy: 0.6573 - val_loss: 1.4000 - val_accuracy: 0.6606\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x7da0645d4940>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model can now be used to translate new English sentences to Spanish, but it's not as simple as just calling `model.predict()`, because the decoder expects the input as words that was predicted at the previous time step. One way to do this is to write a custom memory cell that keeps track of the previous output and feed it to the encoder at the next time step. However, to keep things simple, we can just call the model multiple times, predicting one extra word at each round. Let's write a utility function for that:"
      ],
      "metadata": {
        "id": "SSbRahUnRrqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence_en):\n",
        "    translation = \"\"\n",
        "    for word_idx in range(max_length):\n",
        "        X = tf.convert_to_tensor([sentence_en])  # encoder input\n",
        "        X_dec = tf.convert_to_tensor([\"startofseq \" + translation])  # decoder input\n",
        "        y_proba = model.predict((X, X_dec))[0, word_idx]  # last token's probas\n",
        "        predicted_word_id = np.argmax(y_proba)\n",
        "        predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\n",
        "\n",
        "        if predicted_word == \"[UNK]\":\n",
        "            # Handle [UNK] token\n",
        "            translation += \" <unknown>\"\n",
        "            continue\n",
        "\n",
        "        if predicted_word == \"endofseq\":\n",
        "            break\n",
        "\n",
        "        translation += \" \" + predicted_word\n",
        "\n",
        "    return translation.strip()"
      ],
      "metadata": {
        "id": "uuW4RYgOQy1K"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function simply keeps predicting 1 word at a time, gradually completing the translation. It will stop once it reaches the EOS token:"
      ],
      "metadata": {
        "id": "CDLixRA7TrF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"I like soccer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "hS9Ou0V8Tp5C",
        "outputId": "b9967ff5-3a32-4b13-9a05-761e036e7ac2"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'me gusta el fútbol'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bidirectional RNNs\n",
        "**Directional RNNs** processes input sequences in a single direction — either forward (from past to future) or backward (from future to past). This makes them suitable for tasks like language modeling, where predicting future words relies solely on past context.\n",
        "\n",
        "In contrast, **Bidirectional RNNs** processes sequences in both directions simultaneously, combining information from both the past and future contexts. This allows the model to have a more comprehensive understanding of each input element by considering what comes before and after it.\n",
        "\n",
        "For tasks like text classification or in the encoder of a sequence-to-sequence (seq2seq) model, it is often beneficial to look ahead at future words when encoding a given word. This is because understanding the context of a word is improved when both its preceding and following words are considered. For example, in sentiment analysis, the sentiment of a word can be influenced by the words around it, not just those that precede it. Similarly, in seq2seq models, a bidirectional encoder allows the model to better capture the full context of the input sequence before generating the output, improving overall performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hZYdJvQsV-x7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement a Bidirectional recurrent layer in Keras, just wrap a recurrent layer in a `tf.keras.layers.Bidirectional` layer. For example, the following `Bidirectional` layer could be used as the encoder in the translation model:"
      ],
      "metadata": {
        "id": "Q4ZRBoSppoxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "encoder = tf.keras.layers.Bidirectional(\n",
        "    tf.keras.layers.LSTM(256, return_state=True))"
      ],
      "metadata": {
        "id": "oAsQ8gI0TzsZ"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, the output will now return 4 states instead of 2: the final short-term, long-term states of the forward LSTM layer, the final short-term and long-term states of the backward LSTM layer. To deal with this, we can concatenate the 2 short-term states and concatenate the 2 long-term states:"
      ],
      "metadata": {
        "id": "tmp4fOq7p6PW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
        "encoder_state = [tf.concat(encoder_state[::2], axis=-1),  # short-term (0 & 2)\n",
        "                 tf.concat(encoder_state[1::2], axis=-1)]  # long-term (1 & 3)"
      ],
      "metadata": {
        "id": "WS7cqIgBp3Py"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete the model and train it:"
      ],
      "metadata": {
        "id": "W1ey1yWCqXVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\n",
        "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
        "Y_proba = output_layer(decoder_outputs)\n",
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                       outputs=[Y_proba])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=3,\n",
        "          validation_data=((X_valid, X_valid_dec), Y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCjG4c8TqM01",
        "outputId": "62679ee9-8346-4976-9170-46f997be46f8"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "3125/3125 [==============================] - 103s 29ms/step - loss: 2.1674 - accuracy: 0.5397 - val_loss: 1.5556 - val_accuracy: 0.6317\n",
            "Epoch 2/3\n",
            "3125/3125 [==============================] - 78s 25ms/step - loss: 1.3582 - accuracy: 0.6676 - val_loss: 1.3260 - val_accuracy: 0.6724\n",
            "Epoch 3/3\n",
            "3125/3125 [==============================] - 78s 25ms/step - loss: 1.1321 - accuracy: 0.7110 - val_loss: 1.2388 - val_accuracy: 0.6927\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x7da01fb36410>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"I like soccer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "WTGN4s4gqaZK",
        "outputId": "4d104a71-8753-47d6-dda6-e8b8c4f8fa0f"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 4s 4s/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'me gusta el fútbol'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Mechanisms\n",
        "Consider the path from the word “soccer” to its translation “fútbol”: it is quite\n",
        "long! This means that a representation of this word (along with all the other words) needs to be\n",
        "carried over many steps before it is actually used.\n",
        "\n",
        "Attention mechanisms allow neural networks to **focus on the most relevant parts of input data** when making predictions. Originally introduced for sequence-to-sequence (seq2seq) models in machine translation, attention has since become a core component in many deep learning tasks like text summarization, image captioning, and question answering.\n",
        "\n",
        "In traditional RNNs or LSTMs, the encoder compresses the **entire input sequence into a fixed-length vector**, which can cause performance issues for long sequences. Attention solves this by allowing the decoder to dynamically \"attend\" to different parts of the input sequence at each time step, instead of relying solely on a fixed context vector.\n",
        "\n",
        "**Types of Attention Mechanisms:**\n",
        "- **Bahdanau Attention (Additive)**\n",
        " -  Computes a weighted sum of encoder outputs by learning a score function that measures the importance of each input step.\n",
        "\n",
        "- **Luong Attention (Multiplicative)**\n",
        " - Uses a dot product between the decoder hidden state and encoder outputs to compute attention scores, making it more efficient.\n",
        "\n",
        " Keras provides a `tf.keras.layers.Attention` layer for *Luong attention* and an `AdditiveAttemtopm` layer for *Bahdanau attention*."
      ],
      "metadata": {
        "id": "Wq0gvKjOq4z2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's add the *Luong Attention* to the encoder-decoder model. Since we need to pass all the encoder's outputs to the `Attention` layer, we first need to set `return_sequences=True` when creating the encoder:"
      ],
      "metadata": {
        "id": "7H-7QrSlrWlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "encoder = tf.keras.layers.Bidirectional(\n",
        "    tf.keras.layers.LSTM(256, return_sequences=True, return_state=True))"
      ],
      "metadata": {
        "id": "0kU1dxCbqcrh"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, the output will now return 4 states instead of 2: the final short-term, long-term states of the forward LSTM layer, the final short-term and long-term states of the backward LSTM layer. To deal with this, we can concatenate the 2 short-term states and concatenate the 2 long-term states:"
      ],
      "metadata": {
        "id": "o6YT5tS0MO4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
        "encoder_state = [tf.concat(encoder_state[::2], axis=-1),  # short-term (0 & 2)\n",
        "                 tf.concat(encoder_state[1::2], axis=-1)]  # long-term (1 & 3)\n",
        "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)"
      ],
      "metadata": {
        "id": "cr6ASb1FMQFt"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need to create the attention layer and pass it the decoder's states and the encoder's outputs. However, to access the decoder's states at teach step, we need to write a custom memory cell. For simplicity, use the decoder's ooutputs instead of its states, then pass the attention layer's outputs directly to the output layer:"
      ],
      "metadata": {
        "id": "NDy97P_rsRCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attention_layer = tf.keras.layers.Attention()\n",
        "attention_outputs = attention_layer([decoder_outputs, encoder_outputs])\n",
        "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
        "Y_proba = output_layer(attention_outputs)"
      ],
      "metadata": {
        "id": "OFOtNklvsPHp"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build, compile and train the model:"
      ],
      "metadata": {
        "id": "Lm1VoDg-su64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                       outputs=[Y_proba])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
        "          validation_data=((X_valid, X_valid_dec), Y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsIotDoVyP65",
        "outputId": "3bab919d-c20d-4dae-f08c-7f79220e48a6"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3125/3125 [==============================] - 109s 31ms/step - loss: 2.1671 - accuracy: 0.5525 - val_loss: 1.5066 - val_accuracy: 0.6547\n",
            "Epoch 2/10\n",
            "3125/3125 [==============================] - 84s 27ms/step - loss: 1.3543 - accuracy: 0.6805 - val_loss: 1.3288 - val_accuracy: 0.6858\n",
            "Epoch 3/10\n",
            "3125/3125 [==============================] - 78s 25ms/step - loss: 1.1863 - accuracy: 0.7111 - val_loss: 1.2564 - val_accuracy: 0.7012\n",
            "Epoch 4/10\n",
            "3125/3125 [==============================] - 84s 27ms/step - loss: 1.0727 - accuracy: 0.7328 - val_loss: 1.2301 - val_accuracy: 0.7064\n",
            "Epoch 5/10\n",
            "3125/3125 [==============================] - 80s 25ms/step - loss: 0.9812 - accuracy: 0.7510 - val_loss: 1.2180 - val_accuracy: 0.7104\n",
            "Epoch 6/10\n",
            "3125/3125 [==============================] - 79s 25ms/step - loss: 0.9043 - accuracy: 0.7665 - val_loss: 1.2359 - val_accuracy: 0.7095\n",
            "Epoch 7/10\n",
            "3125/3125 [==============================] - 79s 25ms/step - loss: 0.8383 - accuracy: 0.7802 - val_loss: 1.2503 - val_accuracy: 0.7113\n",
            "Epoch 8/10\n",
            "3125/3125 [==============================] - 78s 25ms/step - loss: 0.7792 - accuracy: 0.7933 - val_loss: 1.2705 - val_accuracy: 0.7117\n",
            "Epoch 9/10\n",
            "3125/3125 [==============================] - 79s 25ms/step - loss: 0.7295 - accuracy: 0.8041 - val_loss: 1.3004 - val_accuracy: 0.7090\n",
            "Epoch 10/10\n",
            "3125/3125 [==============================] - 78s 25ms/step - loss: 0.6855 - accuracy: 0.8134 - val_loss: 1.3261 - val_accuracy: 0.7085\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x7d9fc29568c0>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"I like soccer and also going to the beach\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "iXmo_4cvszex",
        "outputId": "27e4f812-76b7-44e5-cf43-32dacf241e0b"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'me gusta el fútbol y me gusta también a la playa'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is now able to handle much longer sentences since the attention layer provides a way to focus the attention of the model on part of the inputs."
      ],
      "metadata": {
        "id": "IivU6yzVs2rW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hugging Face's Transformation Library\n",
        "Hugging Face is an AI company that has built a whole ecosystem of easy-to-use open source tools for NLP, vision, and beyond. Hugging Face's Transformers library allows us to easily download a pretrained model, including its corresponding tokenizer, and then fine-tune it on our own dataset, if needed."
      ],
      "metadata": {
        "id": "_4X-ivzCOLBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simplest way to use the Transformers library is to use the `transformers.pipeline()` function: just specify which task we want, such as sentiment analysis, and it downloads a default pretrained model that is ready to be used:"
      ],
      "metadata": {
        "id": "D4iQEanyOlCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")  # many other tasks are available\n",
        "result = classifier(\"The actors were very convincing.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLlTvwe2ONBt",
        "outputId": "1fbe34ea-3bf3-4606-e6bf-eb4c326a2a78"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Result` is a Python list containing 1 dictionary per input:"
      ],
      "metadata": {
        "id": "KJd1ERuxO5JV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CS3R6WgUO3VX",
        "outputId": "0cac8f42-1e04-472f-b94b-00f9d89474ef"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9998071789741516}]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models can be very biased, it may like or dislike some countries depending on the data it was trained on, so we must be careful while using it. For example:"
      ],
      "metadata": {
        "id": "b9gvrQD_PJRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier([\"I am from India.\", \"I am from Iraq.\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0SM1928O-wS",
        "outputId": "bb921498-555b-4753-c63a-ca80be7e2d1d"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9896161556243896},\n",
              " {'label': 'NEGATIVE', 'score': 0.9811071157455444}]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `pipeline()` function uses the default model for the given task. For example, for text classification tasks such as sentiment analysis, it defaults to `distilbert-base-uncased-finetuned-sst-2-english` model with an uncased tokenizer, trained on English Wikipedia and a corpus of English books. However, we can manually specify a model that we want as well:"
      ],
      "metadata": {
        "id": "um1S7in0PWfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"huggingface/distilbert-base-uncased-finetuned-mnli\"\n",
        "classifier_mnli = pipeline(\"text-classification\", model=model_name)\n",
        "classifier_mnli(\"She loves me. [SEP] She loves me not.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooWbc5TSPGir",
        "outputId": "9bb8f314-a7d6-4468-c222-4c4de9beaf1b"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'contradiction', 'score': 0.9790192246437073}]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though the pipeline API is very simple and convenient, sometimes we might need more control. For such cases, the Transformers library provides many classes, including all sorts of tokenizers, model configurations, callbacks and many more. For example, let's load the `DistilBERT` model along with its corresponding tokenizer, using the `TFAutoModelForSequenceClassification` and\n",
        "`AutoTokenizer` classes:"
      ],
      "metadata": {
        "id": "_5zEDciLQoTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Crsie35fQgDt",
        "outputId": "f25f86ab-ad36-46aa-89b6-7c023a9b5064"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
            "\n",
            "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's tokenize a couple pairs of sentences. In the following code, we activate padding and specify that we want TensorFlow tensors instead of Python lists:"
      ],
      "metadata": {
        "id": "AZpNif1WRFUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = tokenizer([\"I like soccer. [SEP] We all love soccer!\",\n",
        "                       \"Joe lived for a very long time. [SEP] Joe is old.\"],\n",
        "                      padding=True, return_tensors=\"tf\")\n",
        "token_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2U_pr58SRCd2",
        "outputId": "6a2b5f56-ae2a-4417-e925-6ac29d09f0f2"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\n",
              "array([[ 101, 1045, 2066, 4715, 1012,  102, 2057, 2035, 2293, 4715,  999,\n",
              "         102,    0,    0,    0],\n",
              "       [ 101, 3533, 2973, 2005, 1037, 2200, 2146, 2051, 1012,  102, 3533,\n",
              "        2003, 2214, 1012,  102]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\n",
              "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output is a dictionary-like instance of the `BatchEncoding` class, which contains the sequences of token IDs as well as a cask containing \"0s\" for the padding tokens."
      ],
      "metadata": {
        "id": "Au2_wvbzRSku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we set `return_token_type_ids=True` when calling the tokenizer, we will get an extra tensor that indicates which sentence each token belongs to.\n",
        "\n",
        "Next, we can directly pass this `BatchEncoding` object to the model:"
      ],
      "metadata": {
        "id": "MDeH2GI9RgmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(token_ids)\n",
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Un8X0fSRPy6",
        "outputId": "167ae420-e962-4ead-eabe-4fd46ce2f3de"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
              "array([[-2.1123817 ,  1.17868   ,  1.4100995 ],\n",
              "       [-0.01478346,  1.0962477 , -0.99199575]], dtype=float32)>, hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we apply the softmax activation function to convert these logits to class probabilities, and use the argmax() function to predict the class with the highest probability for each input sentence pair:"
      ],
      "metadata": {
        "id": "Kh1xE6NsRvCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y_probas = tf.keras.activations.softmax(outputs.logits)\n",
        "Y_probas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vpjC9aiRt_k",
        "outputId": "c1bfe0f7-09ca-4f36-c40f-567f5b798422"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
              "array([[0.01619703, 0.43523633, 0.54856664],\n",
              "       [0.2265597 , 0.6881726 , 0.08526774]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred = tf.argmax(Y_probas, axis=1)\n",
        "Y_pred  # 0 = contradiction, 1 = entailment, 2 = neutral"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WUbEMRZR4YC",
        "outputId": "d42e85b0-005a-4b3c-cb9d-c9571a857af8"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2,), dtype=int64, numpy=array([2, 1])>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model correctly classified the 1st sentence pair as neutral and the 2nd sentence pair as entailment."
      ],
      "metadata": {
        "id": "we2C2XyhR-bt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To fine-tune the model, we can train the model as usual with Keras. However, since the model outputs logits instead of probabilities, we must use `tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ` instead of the usual `sparse_categorical_crossentropy`:"
      ],
      "metadata": {
        "id": "Tf55LaKmSINy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [(\"Sky is blue\", \"Sky is red\"), (\"I love her\", \"She loves me\")]\n",
        "X_train = tokenizer(sentences, padding=True, return_tensors=\"tf\").data\n",
        "y_train = tf.constant([0, 2])  # contradiction, neutral\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(loss=loss, optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train, y_train, epochs=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIZ1GhM-R8Jh",
        "outputId": "af3d7283-aa50-4e7b-c689-bb4c75a510dc"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1/1 [==============================] - 49s 49s/step - loss: 0.6666 - accuracy: 0.5000\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.3429 - accuracy: 1.0000\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNjBeYJoMTA003gM6ZQOS24",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}